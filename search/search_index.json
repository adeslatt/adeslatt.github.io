{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\ud83d\udc49 MARCH 13-MARCH 17, 2023 CLICK TO REGISTER \ud83d\udc48 Kids First & INCLUDE Elements of Style in Workflow Creation Gabriella Miller Kids First Pediatric Research Program ( Kids First ) and the [INCLUDE]((https://includedcc.org/) (INvestigation of Co-occuring Conditions Across the Lifespan to Understand Down syndromE) Project are joining forces to offer a 5-day training program designed specifically for researchers. Learn how to use cloud platforms and workflows to ask scientific questions with ease and confidence. Our expert instructors will guide you through the processes that assure reproducibility, repurposability regardless of the computational environment. Achieve greater success with a modular, testable approach to work. In 2018, the NIH launched INCLUDE Project to improve the health and quality of life for people with Down syndrome. The INCLUDE Data Coordinating Center (DCC) was created to give Down Syndrome researchers and the community access to data and analysis tools including a free-to-use resource called the INCLUDE Data Hub . The Data Hub brings information and resources from many sources to gether in one place and allows scientists to study conditions affecting individuals with Down syndrome fastr than ever before. Kids First is a trans-NIH Common Fund program whose goal is to help researchers uncover new insights into the biology of childhood cancer and structural birth defects, including the discovery of shared genetic pathways between these disorders. To achieve this goal, the program has developed the Kids First Data Resource Center, a cloud-based platform which publicly shares genetic and clinical data from 30,000 childhood cancer and structural birth defect patients and families. Through the Kids First Data Resource Portal researchers have access to analysis tools and collaborative research. Both of these platforms are gateways that provide democratizing access to the vast and flexible world of cloud computing, powered by CAVATICA and Velsera (formerly Seven Bridges) Before you attend the course, you need to set up GitHub , Zenodo and ORCID accounts and have basic knowledge of the terminal window, shell environment and the commands for navigation within that environment Course Overview:","title":"Home"},{"location":"#march-13-march-17-2023-click-to-register","text":"","title":"\ud83d\udc49 MARCH 13-MARCH 17, 2023 CLICK TO REGISTER \ud83d\udc48"},{"location":"#kids-first-include-elements-of-style-in-workflow-creation","text":"Gabriella Miller Kids First Pediatric Research Program ( Kids First ) and the [INCLUDE]((https://includedcc.org/) (INvestigation of Co-occuring Conditions Across the Lifespan to Understand Down syndromE) Project are joining forces to offer a 5-day training program designed specifically for researchers. Learn how to use cloud platforms and workflows to ask scientific questions with ease and confidence. Our expert instructors will guide you through the processes that assure reproducibility, repurposability regardless of the computational environment. Achieve greater success with a modular, testable approach to work. In 2018, the NIH launched INCLUDE Project to improve the health and quality of life for people with Down syndrome. The INCLUDE Data Coordinating Center (DCC) was created to give Down Syndrome researchers and the community access to data and analysis tools including a free-to-use resource called the INCLUDE Data Hub . The Data Hub brings information and resources from many sources to gether in one place and allows scientists to study conditions affecting individuals with Down syndrome fastr than ever before. Kids First is a trans-NIH Common Fund program whose goal is to help researchers uncover new insights into the biology of childhood cancer and structural birth defects, including the discovery of shared genetic pathways between these disorders. To achieve this goal, the program has developed the Kids First Data Resource Center, a cloud-based platform which publicly shares genetic and clinical data from 30,000 childhood cancer and structural birth defect patients and families. Through the Kids First Data Resource Portal researchers have access to analysis tools and collaborative research. Both of these platforms are gateways that provide democratizing access to the vast and flexible world of cloud computing, powered by CAVATICA and Velsera (formerly Seven Bridges) Before you attend the course, you need to set up GitHub , Zenodo and ORCID accounts and have basic knowledge of the terminal window, shell environment and the commands for navigation within that environment","title":"Kids First &amp; INCLUDE Elements of Style in Workflow Creation"},{"location":"#course-overview","text":"","title":"Course Overview:"},{"location":"about/attribution/","text":"Acknowledgements CAVATICA is a joint development between Seven Bridges and the Children's Hospital of Philadelphia Seven Bridges supports multiple workflow languages in its application development, including CWL, Nextflow and soon WDL Nextflow workflow information and guidance was gratefully received from Phil Palmer from his classes at the Jackson Laboratory while he was at Lifebit and @cgpu Christina Chatzipantsiou has been my infallible guide. @cgpu Christina Chatzipantsiou taught with me the Dry Bench Skills for the Researchers and Elements of Style class at the ISCB Academy. Common Workflow script guidance was gratefully received from Miguel Brown at Children's Hospital of Philadelphia.","title":"Attribution"},{"location":"about/attribution/#acknowledgements","text":"CAVATICA is a joint development between Seven Bridges and the Children's Hospital of Philadelphia Seven Bridges supports multiple workflow languages in its application development, including CWL, Nextflow and soon WDL Nextflow workflow information and guidance was gratefully received from Phil Palmer from his classes at the Jackson Laboratory while he was at Lifebit and @cgpu Christina Chatzipantsiou has been my infallible guide. @cgpu Christina Chatzipantsiou taught with me the Dry Bench Skills for the Researchers and Elements of Style class at the ISCB Academy. Common Workflow script guidance was gratefully received from Miguel Brown at Children's Hospital of Philadelphia.","title":"Acknowledgements"},{"location":"about/background/","text":"About Over a period of 5 days, only two hours a day, the learner learned elements of style in the construction and containerization of small single-function processes that facilitate repurposable workflow creation and execution. This hands-on-tutorial was given through a webinar using the Kids First Data Resource Center . This repository was used in the course and contains self-learnings to facilitate work. In this repository, contains how these processes may be kept up-to-date and alert the creator to the functional state of these processes (working or failing) by using a feature found within GitHub called GitHub Actions. This hands-on-course will use a small example to provide the structure, philosophy and approach to achieving this desirable outcome. This course seeks to help to demystify and make accessible powerful methods one can use to achieve platform independence and platform interoperability. Using a simple example to demonstrate these techniques, we will break down and walk the learner through each of the construction steps. The learners will be introduced to Conda, Docker, GitHub and the standard workflow language, Nextflow. If time permits, we will also show how these containerized processes can also be represented in a second standard workflow language implementation (e.g. Common Workflow Language or WDL). By the end of the course, the learner will understand these Elements of Style and will know how Conda, Docker, GitHub, Zenodo, and Nextflow enable repurposable research. Moreover, these steps will be on GitHub for the Learner to return to and reproduce themselves after the end of the course. In taking this course, the Learner will also be shown the power of JupyterLab notebooks to facilitate literate programming. Through their participation in the class, learners will learn and understand FAIR (findability, accessibility, interoperability and reusability) best practices. We ask all participants to get a GitHub, Zenodo and ORCID accounts prior to the course. We ask for minimal background knowledge of the command line, simple commands in the shell environment, we enable a bit of self-learning from the repository to facilitate the acquisition of this knowledge. This work was powered on CAVATICA and Kids First Data Resource Center","title":"Background"},{"location":"about/background/#about","text":"Over a period of 5 days, only two hours a day, the learner learned elements of style in the construction and containerization of small single-function processes that facilitate repurposable workflow creation and execution. This hands-on-tutorial was given through a webinar using the Kids First Data Resource Center . This repository was used in the course and contains self-learnings to facilitate work. In this repository, contains how these processes may be kept up-to-date and alert the creator to the functional state of these processes (working or failing) by using a feature found within GitHub called GitHub Actions. This hands-on-course will use a small example to provide the structure, philosophy and approach to achieving this desirable outcome. This course seeks to help to demystify and make accessible powerful methods one can use to achieve platform independence and platform interoperability. Using a simple example to demonstrate these techniques, we will break down and walk the learner through each of the construction steps. The learners will be introduced to Conda, Docker, GitHub and the standard workflow language, Nextflow. If time permits, we will also show how these containerized processes can also be represented in a second standard workflow language implementation (e.g. Common Workflow Language or WDL). By the end of the course, the learner will understand these Elements of Style and will know how Conda, Docker, GitHub, Zenodo, and Nextflow enable repurposable research. Moreover, these steps will be on GitHub for the Learner to return to and reproduce themselves after the end of the course. In taking this course, the Learner will also be shown the power of JupyterLab notebooks to facilitate literate programming. Through their participation in the class, learners will learn and understand FAIR (findability, accessibility, interoperability and reusability) best practices. We ask all participants to get a GitHub, Zenodo and ORCID accounts prior to the course. We ask for minimal background knowledge of the command line, simple commands in the shell environment, we enable a bit of self-learning from the repository to facilitate the acquisition of this knowledge. This work was powered on CAVATICA and Kids First Data Resource Center","title":"About"},{"location":"about/how-this-site-was-made/","text":"How this site was made This site was made and is maintained using MkDocs . It was pretty straight forward to convert the existing GitHub repository structure used for teaching the Elements of Style course. The tricky part was understanding the deployment aspect. I did this by understanding the two different functions of this repository. Location for teaching and using MarkDown for the information Website for navigation of the content. Both of the different functions require their own repositories. The first repository is the content repository where the information is maintained, added to and altered. The second repository is the GitHub pages repository where the html information is generated and deployed. Because this repository is auto-generated, it is important not to edit there, however it still needs to exist. This was a difficult concept for me to wrap my head around. Kids First & INCLUDE Elements of Style Content Kids First & INCLUDE Elements of Style WebSite","title":"Making this site"},{"location":"about/how-this-site-was-made/#how-this-site-was-made","text":"This site was made and is maintained using MkDocs . It was pretty straight forward to convert the existing GitHub repository structure used for teaching the Elements of Style course. The tricky part was understanding the deployment aspect. I did this by understanding the two different functions of this repository. Location for teaching and using MarkDown for the information Website for navigation of the content. Both of the different functions require their own repositories. The first repository is the content repository where the information is maintained, added to and altered. The second repository is the GitHub pages repository where the html information is generated and deployed. Because this repository is auto-generated, it is important not to edit there, however it still needs to exist. This was a difficult concept for me to wrap my head around.","title":"How this site was made"},{"location":"about/how-this-site-was-made/#kids-first-include-elements-of-style-content","text":"","title":"Kids First &amp; INCLUDE Elements of Style Content"},{"location":"about/how-this-site-was-made/#kids-first-include-elements-of-style-website","text":"","title":"Kids First &amp; INCLUDE Elements of Style WebSite"},{"location":"about/license/","text":"Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION Definitions. \"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document. \"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License. \"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License. \"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files. \"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types. \"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below). \"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof. \"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\" \"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions: (a) You must give any other recipients of the Work or Derivative Works a copy of this License; and (b) You must cause any modified files to carry prominent notices stating that You changed the files; and (c) You must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and (d) If the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability. END OF TERMS AND CONDITIONS APPENDIX: How to apply the Apache License to your work. To apply the Apache License to your work, attach the following boilerplate notice, with the fields enclosed by brackets \"[]\" replaced with your own identifying information. (Don't include the brackets!) The text should be enclosed in the appropriate comment syntax for the file format. We also recommend that a file or class name and description of purpose be included on the same \"printed page\" as the copyright notice for easier identification within third-party archives. Copyright [yyyy] [name of copyright owner] Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"License"},{"location":"about/other-topics-of-interest/","text":"Background Information and other Topics of Interest Anaconda Package Jupytext CAVATICA Data Studio JupyterLab Startup INCLUDE DataHub Login with ORCID Shell Google Cloud","title":"Other topics of interest"},{"location":"about/other-topics-of-interest/#background-information-and-other-topics-of-interest","text":"Anaconda Package Jupytext CAVATICA Data Studio JupyterLab Startup INCLUDE DataHub Login with ORCID Shell Google Cloud","title":"Background Information and other Topics of Interest"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/","text":"A Few Simple Rules - Shortened Edition Disclaimer The views expressed in this course represent the views of Anne Deslattes Mays, PhD and do not represent the views of NICHD, NIH or the United States Government. In what follows - is my development of a practice that enables workflow and platform independence facilitating repurposability. Learning from those who have walked the journey Elements of Programming Style B. W. Kernighan and P. J. Plauger, The Elements of Programming Style 2nd Edition, McGraw Hill, New York, 1978. ISBN 0-07-034207-5 Quotes by P. J. Plauger The year was 1919, the first World War was at its close and a student, E. B. White, took a course, English 8, taught by William Strunk Jr. The course featured a required textbook, a slim volume called The Elements of Style . The durability of this slim book informed the development of the book, The Elements of Programming Style by Kernigan and Plauger, whose lessons we adapt here in this course. Showing again the durability of the approach of beginning with philosophy as one approaches their work and use of programs and structure to achieve their work. So with this nod to E.B. White, William Strunk, Jr, Brian Kernigan and Plauger, we begin with our own Lessons and Pithy Phrases. What is a Pithy Phrase: A pithy phrase or statement is brief but full of substance and meaning Lessons Translated to the Workflow/Containerized Process (Truncated Pithy Phrases) Top 10 Lessons Number Lesson Relates Too Full Lessons Number 1. Write clearly \u2013 don't be too clever. Everything 1. 2. Write first in easy-to-understand pseudo language Everything 13. 3. Modularize. Use procedures and functions. Containerizing with Dockerfile 14. 4. Write and test in small pieces. Use Dockerfiles, GitHub Actions to test 17. 5. Test inputs and outputs Test on command line workflows are input, process, output 19. 6. Use debugging compilers. Compiling with workflows 29. 7. Make it right before you make it faster. Everything 30. 8. Make sure special cases are truly special. Everythng 35. 9. Keep it simple to make it faster. Everything 36. 10. Document your process in GitHub use workflow languages, containers and GitHub 45. A Few Simple Rules - Full List A Few Simple Rules Return to today's Agenda","title":"A Few Simple Rules Shortened"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#a-few-simple-rules-shortened-edition","text":"","title":"A Few Simple Rules - Shortened Edition"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#disclaimer","text":"The views expressed in this course represent the views of Anne Deslattes Mays, PhD and do not represent the views of NICHD, NIH or the United States Government. In what follows - is my development of a practice that enables workflow and platform independence facilitating repurposability.","title":"Disclaimer"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#learning-from-those-who-have-walked-the-journey","text":"Elements of Programming Style B. W. Kernighan and P. J. Plauger, The Elements of Programming Style 2nd Edition, McGraw Hill, New York, 1978. ISBN 0-07-034207-5 Quotes by P. J. Plauger The year was 1919, the first World War was at its close and a student, E. B. White, took a course, English 8, taught by William Strunk Jr. The course featured a required textbook, a slim volume called The Elements of Style . The durability of this slim book informed the development of the book, The Elements of Programming Style by Kernigan and Plauger, whose lessons we adapt here in this course. Showing again the durability of the approach of beginning with philosophy as one approaches their work and use of programs and structure to achieve their work. So with this nod to E.B. White, William Strunk, Jr, Brian Kernigan and Plauger, we begin with our own Lessons and Pithy Phrases. What is a Pithy Phrase: A pithy phrase or statement is brief but full of substance and meaning","title":"Learning from those who have walked the journey"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#lessons-translated-to-the-workflowcontainerized-process-truncated-pithy-phrases","text":"Top 10 Lessons Number Lesson Relates Too Full Lessons Number 1. Write clearly \u2013 don't be too clever. Everything 1. 2. Write first in easy-to-understand pseudo language Everything 13. 3. Modularize. Use procedures and functions. Containerizing with Dockerfile 14. 4. Write and test in small pieces. Use Dockerfiles, GitHub Actions to test 17. 5. Test inputs and outputs Test on command line workflows are input, process, output 19. 6. Use debugging compilers. Compiling with workflows 29. 7. Make it right before you make it faster. Everything 30. 8. Make sure special cases are truly special. Everythng 35. 9. Keep it simple to make it faster. Everything 36. 10. Document your process in GitHub use workflow languages, containers and GitHub 45.","title":"Lessons Translated to the Workflow/Containerized Process (Truncated Pithy Phrases)"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#a-few-simple-rules-full-list","text":"A Few Simple Rules","title":"A Few Simple Rules - Full List"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules-Shortened/#return-to-todays-agenda","text":"","title":"Return to today's Agenda"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/","text":"A Few Simple Rules Disclaimer The views expressed in this course represent the views of Anne Deslattes Mays, PhD and do not represent the views of NICHD, NIH or the United States Government. In what follows - is my development of a practice that enables workflow and platform independence facilitating reproducibility. Learning from those who have walked the journey Elements of Programming Style B. W. Kernighan and P. J. Plauger, The Elements of Programming Style 2nd Edition, McGraw Hill, New York, 1978. ISBN 0-07-034207-5 Quotes by P. J. Plauger The year was 1919, the first World War was at its close and a student, E. B. White, took a course, English 8, taught by William Strunk Jr. The course featured a required textbook, a slim volume called The Elements of Style . The durability of this slim book informed the development of the book, The Elements of Programming Style by Kernigan and Plauger, whose lessons we adapt here in this course. Showing again the durability of the approach of beginning with philosophy as one approaches their work and use of programs and structure to achieve their work. So with this nod to E.B. White, William Strunk, Jr, Brian Kernigan and Plauger, we begin with our own Lessons and Pithy Phrases. Lessons Translated to the Workflow/Containerized Process (Truncated Pithy Phrases) Where I have made modifications to these Pithy Phrases to the map to what we are teaching here will be in italized and emphasized Its lessons are summarized at the end of each section in pithy maxims, such as \"Let the machine do the dirty work\": Write clearly \u2013 don't be too clever. timeless Say what you mean, simply and directly. timeless Use containerized processes (in a way similar to library functions) whenever feasible. Avoid too many temporary variables. Write clearly \u2013 don't sacrifice clarity for efficiency. timeless Let the machine do the dirty work. timeless Replace repetitive expressions by calls to common functions. timeless - when you start to see yourself do this - replace with a single function Parenthesize to avoid ambiguity. 9 Choose variable names that won't be confused. timeless Avoid unnecessary branches. If a logical expression is hard to understand, try transforming it. timeless Choose a data representation that makes the program simple. Write first in easy-to-understand pseudo language; then translate into whatever language you have to use. timeless Modularize. Use procedures and functions. and containerize - use GitHub Actions to build, test and keep up-to-date Avoid gotos completely if you can keep the program readable. Don't patch bad code \u2013 rewrite it. Write and test a big program in small pieces. timeless this can be done by the use of these tested, dockerized processes Use recursive procedures for recursively-defined data structures. Test input for plausibility and validity. timeless - make sure you understand the source of your data Make sure input doesn't violate the limits of the program. Terminate input by end-of-file marker, not by count. Identify bad input; recover if possible. Make input easy to prepare and output self-explanatory. Use uniform input formats. Make input easy to proofread. timeless Use self-identifying input. Allow defaults. Echo both on output. Make sure all variables are initialized before use. Don't stop at one bug. Use debugging compilers. timeless this is different with workflow languages - you can test each of the steps in the workflow verifying inputs, outputs and processes - dockerizing, testing Watch out for off-by-one errors. timeless Take care to branch the right way on equality. Be careful if a loop exits to the same place from the middle and the bottom. Make sure your code does \"nothing\" gracefully. Test programs at their boundary values. Check some answers by hand. timeless 10.0 times 0.1 is hardly ever 1.0. timeless always aim for simplicity 7/8 is zero while 7.0/8.0 is not zero. timeless but this would be better for an R or a Python class Don't compare floating point numbers solely for equality. timeless but this would be better for an R or a Python class Make it right before you make it faster. timeless for everything Make it fail-safe before you make it faster. timeless Make it clear before you make it faster. timeless Don't sacrifice clarity for small gains in efficiency. timeless Let your compiler do the simple optimizations. `again, for our world of platforms, let your platform help you - Platforms as a Service, such as CAVATICA by Seven Bridges and CloudOS by Lifebit Don't strain to re-use code; reorganize instead. timeless the more you perform a task, the simpler you see how to get it done, exploit that simplicity and rewrite Make sure special cases are truly special. timeless Keep it simple to make it faster. timeless Don't diddle code to make it faster \u2013 find a better algorithm. timeless find another Bioinformatics algorithm, collaborate, give attribution and expand your reach Instrument your programs. Measure before making efficiency changes. timeless - this means if you introduce changes - are they appropriate Make sure comments and code agree. timeless Don't just echo the code with comments \u2013 make every comment count. timeless Don't comment bad code \u2013 rewrite it. timeless Use variable names that mean something. Use statement labels that mean something. Format a program to help the reader understand it. timeless Document your data layouts. timeless Don't over-comment. timeless Other useful and informative points","title":"A Few Simple Rules"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/#a-few-simple-rules","text":"","title":"A Few Simple Rules"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/#disclaimer","text":"The views expressed in this course represent the views of Anne Deslattes Mays, PhD and do not represent the views of NICHD, NIH or the United States Government. In what follows - is my development of a practice that enables workflow and platform independence facilitating reproducibility.","title":"Disclaimer"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/#learning-from-those-who-have-walked-the-journey","text":"Elements of Programming Style B. W. Kernighan and P. J. Plauger, The Elements of Programming Style 2nd Edition, McGraw Hill, New York, 1978. ISBN 0-07-034207-5 Quotes by P. J. Plauger The year was 1919, the first World War was at its close and a student, E. B. White, took a course, English 8, taught by William Strunk Jr. The course featured a required textbook, a slim volume called The Elements of Style . The durability of this slim book informed the development of the book, The Elements of Programming Style by Kernigan and Plauger, whose lessons we adapt here in this course. Showing again the durability of the approach of beginning with philosophy as one approaches their work and use of programs and structure to achieve their work. So with this nod to E.B. White, William Strunk, Jr, Brian Kernigan and Plauger, we begin with our own Lessons and Pithy Phrases.","title":"Learning from those who have walked the journey"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/#lessons-translated-to-the-workflowcontainerized-process-truncated-pithy-phrases","text":"Where I have made modifications to these Pithy Phrases to the map to what we are teaching here will be in italized and emphasized Its lessons are summarized at the end of each section in pithy maxims, such as \"Let the machine do the dirty work\": Write clearly \u2013 don't be too clever. timeless Say what you mean, simply and directly. timeless Use containerized processes (in a way similar to library functions) whenever feasible. Avoid too many temporary variables. Write clearly \u2013 don't sacrifice clarity for efficiency. timeless Let the machine do the dirty work. timeless Replace repetitive expressions by calls to common functions. timeless - when you start to see yourself do this - replace with a single function Parenthesize to avoid ambiguity. 9 Choose variable names that won't be confused. timeless Avoid unnecessary branches. If a logical expression is hard to understand, try transforming it. timeless Choose a data representation that makes the program simple. Write first in easy-to-understand pseudo language; then translate into whatever language you have to use. timeless Modularize. Use procedures and functions. and containerize - use GitHub Actions to build, test and keep up-to-date Avoid gotos completely if you can keep the program readable. Don't patch bad code \u2013 rewrite it. Write and test a big program in small pieces. timeless this can be done by the use of these tested, dockerized processes Use recursive procedures for recursively-defined data structures. Test input for plausibility and validity. timeless - make sure you understand the source of your data Make sure input doesn't violate the limits of the program. Terminate input by end-of-file marker, not by count. Identify bad input; recover if possible. Make input easy to prepare and output self-explanatory. Use uniform input formats. Make input easy to proofread. timeless Use self-identifying input. Allow defaults. Echo both on output. Make sure all variables are initialized before use. Don't stop at one bug. Use debugging compilers. timeless this is different with workflow languages - you can test each of the steps in the workflow verifying inputs, outputs and processes - dockerizing, testing Watch out for off-by-one errors. timeless Take care to branch the right way on equality. Be careful if a loop exits to the same place from the middle and the bottom. Make sure your code does \"nothing\" gracefully. Test programs at their boundary values. Check some answers by hand. timeless 10.0 times 0.1 is hardly ever 1.0. timeless always aim for simplicity 7/8 is zero while 7.0/8.0 is not zero. timeless but this would be better for an R or a Python class Don't compare floating point numbers solely for equality. timeless but this would be better for an R or a Python class Make it right before you make it faster. timeless for everything Make it fail-safe before you make it faster. timeless Make it clear before you make it faster. timeless Don't sacrifice clarity for small gains in efficiency. timeless Let your compiler do the simple optimizations. `again, for our world of platforms, let your platform help you - Platforms as a Service, such as CAVATICA by Seven Bridges and CloudOS by Lifebit Don't strain to re-use code; reorganize instead. timeless the more you perform a task, the simpler you see how to get it done, exploit that simplicity and rewrite Make sure special cases are truly special. timeless Keep it simple to make it faster. timeless Don't diddle code to make it faster \u2013 find a better algorithm. timeless find another Bioinformatics algorithm, collaborate, give attribution and expand your reach Instrument your programs. Measure before making efficiency changes. timeless - this means if you introduce changes - are they appropriate Make sure comments and code agree. timeless Don't just echo the code with comments \u2013 make every comment count. timeless Don't comment bad code \u2013 rewrite it. timeless Use variable names that mean something. Use statement labels that mean something. Format a program to help the reader understand it. timeless Document your data layouts. timeless Don't over-comment. timeless","title":"Lessons Translated to the Workflow/Containerized Process (Truncated Pithy Phrases)"},{"location":"classes/day-1-reasoning/A-Few-Simple-Rules/#other-useful-and-informative-points","text":"","title":"Other useful and informative points"},{"location":"classes/day-1-reasoning/creating-a-cavatica-account/","text":"Creating a CAVATICA Account Please create your account on CAVATICA Navigate to CAVATICA First screen you will see: You can either select Create an account or Log in with eRA Commons . If you select Create an account , you will see: Or if you select Log in with eRACommons you will see: See further documentation: CAVATICA Account Login Creation Documentation","title":"Creating a CAVATICA Account"},{"location":"classes/day-1-reasoning/creating-a-cavatica-account/#creating-a-cavatica-account","text":"Please create your account on CAVATICA Navigate to CAVATICA First screen you will see: You can either select Create an account or Log in with eRA Commons . If you select Create an account , you will see: Or if you select Log in with eRACommons you will see: See further documentation: CAVATICA Account Login Creation Documentation","title":"Creating a CAVATICA Account"},{"location":"classes/day-1-reasoning/day-1-reasoning/","text":"Agenda for the Day 1: Reasoning Time (UTC) Programme 11.00 - 11:30 Welcome Address and Presentation 11.30 - 11.45 A few simple rules for easier workflow maintenance and reuse 11:45 - 12:00 Lets Dive In, Create an Account, Start a JupyterLab Notebook 12.00 - 12.10 Short Break 12.10 - 12.25 Introduction to the Command-line 12.25 - 12.50 Example Volcano Plot on CAVATICA 12.50 - 13:00 Wrap up and overview of next days topics Additional Resources: NIH Kids First Cloud Credit Program Overview NIH INCLUDE Cloud Credit Program Overview How to apply for Kids First Cloud Credits How to apply for INCLUDE Cloud Credits Joint Kids First and INCLUDE with Velsera (formerly Seven Bridges) Monthly User Support Office Hours - every third Thursday of the month JupyterLab Notebook Conversions The Jupyter Text - do you like working in R Studio with R-markdown? You can convert easily JupyterLab notebooks with R kernels to R Script or R markdown. Explore the concept here: - Jupytext Supported Languages - Jupytext Command Line","title":"Day 1 - Reasoning"},{"location":"classes/day-1-reasoning/day-1-reasoning/#agenda-for-the-day-1-reasoning","text":"Time (UTC) Programme 11.00 - 11:30 Welcome Address and Presentation 11.30 - 11.45 A few simple rules for easier workflow maintenance and reuse 11:45 - 12:00 Lets Dive In, Create an Account, Start a JupyterLab Notebook 12.00 - 12.10 Short Break 12.10 - 12.25 Introduction to the Command-line 12.25 - 12.50 Example Volcano Plot on CAVATICA 12.50 - 13:00 Wrap up and overview of next days topics","title":"Agenda for the Day 1: Reasoning"},{"location":"classes/day-1-reasoning/day-1-reasoning/#additional-resources","text":"NIH Kids First Cloud Credit Program Overview NIH INCLUDE Cloud Credit Program Overview How to apply for Kids First Cloud Credits How to apply for INCLUDE Cloud Credits Joint Kids First and INCLUDE with Velsera (formerly Seven Bridges) Monthly User Support Office Hours - every third Thursday of the month","title":"Additional Resources:"},{"location":"classes/day-1-reasoning/day-1-reasoning/#jupyterlab-notebook-conversions","text":"The Jupyter Text - do you like working in R Studio with R-markdown? You can convert easily JupyterLab notebooks with R kernels to R Script or R markdown. Explore the concept here: - Jupytext Supported Languages - Jupytext Command Line","title":"JupyterLab Notebook Conversions"},{"location":"classes/day-1-reasoning/lets-dive-in/","text":"Let's Dive In So we can be ready with a running notebook, lets get a machine up and running - once up - it will stay up for 30 minutes -- challenging us to keep to our agenda! Lets see how we do! FAST Track Create an Account with CAVATICA If you have not already done so, please go ahead and create an account on CAVATICA . Any difficulties reach out to David in the Zoom Chat. We have an hour after each class this week, where we will stay to assist. Log in straight to CAVATICA Step-by-Step Instructions to log into CAVATICA Step-by-Step Instructions to create a project and start a JupyterLab Notebook While things start to cook -- let me review the Agenda and show a brief presentation If not yet registered If you have not already registered with Kids First, INCLUDE and CAVATICA, please stay after today and we will get you caught up. Registering with Kids First, INCLUDE and other accounts Return to Day 1 Return to today's agenda","title":"Lets dive in"},{"location":"classes/day-1-reasoning/lets-dive-in/#lets-dive-in","text":"So we can be ready with a running notebook, lets get a machine up and running - once up - it will stay up for 30 minutes -- challenging us to keep to our agenda! Lets see how we do!","title":"Let's Dive In"},{"location":"classes/day-1-reasoning/lets-dive-in/#fast-track-create-an-account-with-cavatica","text":"If you have not already done so, please go ahead and create an account on CAVATICA . Any difficulties reach out to David in the Zoom Chat. We have an hour after each class this week, where we will stay to assist.","title":"FAST Track Create an Account with CAVATICA"},{"location":"classes/day-1-reasoning/lets-dive-in/#log-in-straight-to-cavatica","text":"Step-by-Step Instructions to log into CAVATICA Step-by-Step Instructions to create a project and start a JupyterLab Notebook While things start to cook -- let me review the Agenda and show a brief presentation","title":"Log in straight to CAVATICA"},{"location":"classes/day-1-reasoning/lets-dive-in/#if-not-yet-registered","text":"If you have not already registered with Kids First, INCLUDE and CAVATICA, please stay after today and we will get you caught up. Registering with Kids First, INCLUDE and other accounts","title":"If not yet registered"},{"location":"classes/day-1-reasoning/lets-dive-in/#return-to-day-1","text":"Return to today's agenda","title":"Return to Day 1"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/","text":"Logging into CAVATICA Tutorial Login to CAVATICA Step-by-Step Step 1 - Navigate to Cavatica Navigate in your BROWSER (Chrome prefered) https://cavatica.sbgenomics.com/home The CAVATICA Initial Login Window looks like this: Logging in with your eRACommons ID will allow CAVATICA to provide you access to data for which you are authorized. You can Login with the eRA Commons after you have created an account on Cavatica. The account will automatically linked to the eRA Commons, making it possible to Log in with the eRA Commons. See the CAVATICA platform documentation for details You can read about the connection between dbGaP and eRACommons Step 2 - Authorize CAVATICA The next window you will see is the Gen3 login window - where you authorize NIH and RAS to retrieve the controlled-access datasets to which you have access. Click Yes, I authorize The next window can vary depending upon if this is your first time logging in or not. An example of your next window is the Dashboard view Next Step - Create a Project and Start a JupyterLab Notebook Create a Project and Start a JupyterLab Notebook","title":"Logging into CAVATICA Tutorial"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/#logging-into-cavatica-tutorial","text":"","title":"Logging into CAVATICA Tutorial"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/#login-to-cavatica-step-by-step","text":"","title":"Login to CAVATICA Step-by-Step"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/#step-1-navigate-to-cavatica","text":"Navigate in your BROWSER (Chrome prefered) https://cavatica.sbgenomics.com/home The CAVATICA Initial Login Window looks like this: Logging in with your eRACommons ID will allow CAVATICA to provide you access to data for which you are authorized. You can Login with the eRA Commons after you have created an account on Cavatica. The account will automatically linked to the eRA Commons, making it possible to Log in with the eRA Commons. See the CAVATICA platform documentation for details You can read about the connection between dbGaP and eRACommons","title":"Step 1 - Navigate to Cavatica"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/#step-2-authorize-cavatica","text":"The next window you will see is the Gen3 login window - where you authorize NIH and RAS to retrieve the controlled-access datasets to which you have access. Click Yes, I authorize The next window can vary depending upon if this is your first time logging in or not. An example of your next window is the Dashboard view","title":"Step 2 - Authorize CAVATICA"},{"location":"classes/day-1-reasoning/logging-into-cavatica-step-by-step/#next-step-create-a-project-and-start-a-jupyterlab-notebook","text":"Create a Project and Start a JupyterLab Notebook","title":"Next Step - Create a Project and Start a JupyterLab Notebook"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/","text":"To begin, your screen should look like this: This is a JupyterLab environment. Project Jupyter is a non-profit, open-source project, born out of the IPython Project in 2014 as it evolved to support interactive data science and scientific computing across all programming languages. Jupyter will always be 100% open-source software, free for all to use and released under the liberal terms of the modified BSD license. I encourage you to explore Project Jupyter and understand the directions they are going in and how they support open, transparent, re-purposeable and literate programming. The JupyterLab Notebook environment supports at this point 40 languages, in kernels, providing a flexible way to achieve not only the ends of what we are trying to do with analysis, but in a well documented, clear and literate manner. Now that we are at the terminal window, we need to bring our lesson in hand -- and to do that, we are going to begin with forking our GitHub repository. Clone a GitHub Repository We are going to clone a GitHub repository - to do so navigate in your browser to Example Volcano Plot Notebook Copy the Link to Clone the repository Next, navigate to the \ud83d\udfe6 Code button on the right and select HTTPS and copy the link. Clone the repository in the Jupyterlab terminal window Return to your CAVATICA Window and go back to your JupyterLab notebook. And select the terminal window Now we want to clone the repository fork we made: At the prompt type: git clone https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook.git Now let's use our cd command to change into the appropriate directory. cd exampleVolcanoPlotNotebook Recap In this lesson: * We logged into CAVATICA * We started an interactive analysis with JupyterLab * We authenticated with GitHub * We executed and saw some Command Line Functions * We generated a Volcano Plot in a JupyterLab notebook running an R kernel Additional resources: CAVATICA documentation for the JupyterLab interface: https://docs.cavatica.org/docs/editor-quick-reference Official documentation from project JupyterLab: https://jupyterlab.readthedocs.io/en/stable/","title":"Running a jupyterlab notebook"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/#clone-a-github-repository","text":"We are going to clone a GitHub repository - to do so navigate in your browser to Example Volcano Plot Notebook","title":"Clone a GitHub Repository"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/#copy-the-link-to-clone-the-repository","text":"Next, navigate to the \ud83d\udfe6 Code button on the right and select HTTPS and copy the link.","title":"Copy the Link to Clone the repository"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/#clone-the-repository-in-the-jupyterlab-terminal-window","text":"Return to your CAVATICA Window and go back to your JupyterLab notebook. And select the terminal window Now we want to clone the repository fork we made: At the prompt type: git clone https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook.git Now let's use our cd command to change into the appropriate directory. cd exampleVolcanoPlotNotebook","title":"Clone the repository in the Jupyterlab terminal window"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/#recap","text":"In this lesson: * We logged into CAVATICA * We started an interactive analysis with JupyterLab * We authenticated with GitHub * We executed and saw some Command Line Functions * We generated a Volcano Plot in a JupyterLab notebook running an R kernel","title":"Recap"},{"location":"classes/day-1-reasoning/running-a-jupyterlab-notebook/#additional-resources","text":"CAVATICA documentation for the JupyterLab interface: https://docs.cavatica.org/docs/editor-quick-reference Official documentation from project JupyterLab: https://jupyterlab.readthedocs.io/en/stable/","title":"Additional resources:"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/","text":"Starting A JupyterLab Notebook To start a notebook , we need a project . Please let David know your username in the Zoom Chat so you can be added to the elements-of-style-kids-first billing group. Create a new project We are at the login page, please select so we can create a new project. We need to select three pieces of important inforrmation. Enter a name, here I have used Kids-First-INCLUDE-eos - note you will see in the project URL how it uniquely is identified because it includes your user id . Select the appropriate Billing group. For this course, give David your userid and you will be added so you can select EoS-Include-KidsFirst . Select Allow network access \u2757Important\u2757 be sure that you select Allow network access Start a new Analysis Next, within this new project, we will start an analysis. To do this, select Data Studio and select to create a new analysis. Name the analysis, here I used Kids-First-INCLUDE-eos-analysis . Next select . Look over what you have entered When satisfied that it is correct, press Confirm it is starting Upon success, we will see it is starting, lets take a small break. After we will begin with the Command Line Return to Day 1 Agenda","title":"Starting a jupyter lab notebook"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/#starting-a-jupyterlab-notebook","text":"To start a notebook , we need a project . Please let David know your username in the Zoom Chat so you can be added to the elements-of-style-kids-first billing group.","title":"Starting A JupyterLab Notebook"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/#create-a-new-project","text":"We are at the login page, please select so we can create a new project. We need to select three pieces of important inforrmation. Enter a name, here I have used Kids-First-INCLUDE-eos - note you will see in the project URL how it uniquely is identified because it includes your user id . Select the appropriate Billing group. For this course, give David your userid and you will be added so you can select EoS-Include-KidsFirst . Select Allow network access \u2757Important\u2757 be sure that you select Allow network access","title":"Create a new project"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/#start-a-new-analysis","text":"Next, within this new project, we will start an analysis. To do this, select Data Studio and select to create a new analysis. Name the analysis, here I used Kids-First-INCLUDE-eos-analysis . Next select . Look over what you have entered When satisfied that it is correct, press","title":"Start a new Analysis"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/#confirm-it-is-starting","text":"Upon success, we will see it is starting, lets take a small break.","title":"Confirm it is starting"},{"location":"classes/day-1-reasoning/starting-a-jupyter-lab-notebook/#after-we-will-begin-with-the-command-line","text":"Return to Day 1 Agenda","title":"After   we will begin with the Command Line"},{"location":"classes/day-1-reasoning/using-the-command-line/","text":"Using the command line Because I want to make this course accessible to researchers/clinicians who may or may not have had previous command line experience, I recently asked chatGPT the following question: how would you explain \"what is a terminal window\" and \"what is a command line\" to someone naive to these terms? ChatGPT had the following response A terminal window is a type of computer program that allows you to interact with your computer using text commands instead of clicking on icons or menus with a mouse. It's a way to access the \"guts\" of your computer and execute specific instructions. A command line is the text interface in a terminal window where you can enter these instructions or commands. Essentially, it's a way to communicate with your computer using a language that it understands. Instead of pointing and clicking with a mouse, you type out specific commands to make things happen. For example, if you want to create a new folder, you could type \"mkdir foldername\" in the command line, and the computer would create a new folder with the name you specified. Or if you want to open a specific program, you could type its name followed by \".exe\" and press enter, and the program would launch. The command line can be a powerful tool for accomplishing tasks quickly and efficiently, but it does require some knowledge of specific commands and syntax. However, there are many resources available online for learning how to use the command line, and many people find it to be a valuable skill to have. However, there are many resources available online for learning how to use the command line, and many people find it to be a valuable skill to have. We begin by using a tool ExlainShell Utilising the ExplainShell interactive command explorer There is an excellent resource that provides an interactive command explainer that can be accessed at ExplainShell Feel free to use and paste the following commands from the tutorial to see an explanation of the command based on the official commands documentation. 1. List files in the current folder Directory is used interchangeably with the term folder. ls -l Inside ExplainShell you will see the following explanation: 2. Inspect your current folder location in the file system To see in which folder you are currently in, use the pwd command, short for print working directory . This will return the absolut path to the location you currently stand. NOTE: What is an absolute path? what is a relative path? See this resource for a thorough explanation Again feel free now to copy and paste this command in the ExplainShell pwd 3. Copy a file Now let's go to a terminal window inside of the JupyterLab notebook you have just started up. Select terminal Then you will see the command-line: Before we copy a file, let's use a command-line command echo and create a file with the content \"I am the header of your first README.md\". If you want to know about this command, look it up on ExplainShell . We are also using a command called a redirect -- this will use it to put content into a new file. echo 'I am the header of your first README.md' > README.md You can run ls -l to see that the file is there. As well as see the commands more and another command called less . Use ExplainShell to know about these commands, but they show the contents of the file as you might expect. Now let's create a copy: cp README.md duplicated_README.md Now let's inspect if the file was created, by listing files in the directory with the ls -l command. We are expecting to view the file with name duplicated_README.md in the listed files and folders ls -l 4. Deleting a file To delete (remove) a file we can use the command rm name_of_folder . Let's delete the duplicate README.md file named duplicated_README.md rm duplicated_README.md Now let's inspect if the file was created, by listing files in the directory with the ls -l command. We are expecting to not view the file with name duplicated_README.md as we just deleted it in the listed files and folders ls -l 5. Create a new directory (folder) To create a new folder if that folder doesn't exist, we can use the command mkdir name_of_folder . Let's create a folder named new_folder mkdir new_folder Now let's inspect if the folder was created, by listing files in the directory with the ls -l command. We are expecting to view the folder with name new_folder in the listed files and folders Let's inspect the folder exists with our trusted ls -l command ls -l 6. Delete a folder We can delete a folder using the rm -r command: rm -r new_folder Let's inspect that the folder is indeed deleted: ls -l 7. Downloading a file using a link We can retrieve a file using a public link. For example, we have uploaded in a public ZENODO record for this course an example file. The link for the file is: https://zenodo.org/record/4302133/files/deseq2_5k.csv Details of this file, which is a selected subset of the results of running DESeq on RNA-seq files, only those that were with an appropriate p-value (< 0.05) and above the 1.5 fold difference were put into the matrix. https://zenodo.org/record/4302133. You can see the version history, this file was created by a colleague at Lifebit, for the first time this course was offered in 2019 at the Jackson Laboratory. To retrieve this file, we are going to open the terminal window and run the command: cd /sbgenomics/workspace/Kids-First-Elements-of-Style-Workflow-Creation-Maintenance/classes/Running-a-JupyterLab-Notebook wget https://zenodo.org/record/4302133/files/deseq2_5k.csv So please copy this command and paste it in the terminal window and press return - in preparation for the next section of todays class. Reading data and plotting a volcano plot. Return to Day 1 Return to today's agenda","title":"Using the command line"},{"location":"classes/day-1-reasoning/using-the-command-line/#using-the-command-line","text":"Because I want to make this course accessible to researchers/clinicians who may or may not have had previous command line experience, I recently asked chatGPT the following question: how would you explain \"what is a terminal window\" and \"what is a command line\" to someone naive to these terms? ChatGPT had the following response A terminal window is a type of computer program that allows you to interact with your computer using text commands instead of clicking on icons or menus with a mouse. It's a way to access the \"guts\" of your computer and execute specific instructions. A command line is the text interface in a terminal window where you can enter these instructions or commands. Essentially, it's a way to communicate with your computer using a language that it understands. Instead of pointing and clicking with a mouse, you type out specific commands to make things happen. For example, if you want to create a new folder, you could type \"mkdir foldername\" in the command line, and the computer would create a new folder with the name you specified. Or if you want to open a specific program, you could type its name followed by \".exe\" and press enter, and the program would launch. The command line can be a powerful tool for accomplishing tasks quickly and efficiently, but it does require some knowledge of specific commands and syntax. However, there are many resources available online for learning how to use the command line, and many people find it to be a valuable skill to have. However, there are many resources available online for learning how to use the command line, and many people find it to be a valuable skill to have. We begin by using a tool ExlainShell","title":"Using the command line"},{"location":"classes/day-1-reasoning/using-the-command-line/#utilising-the-explainshell-interactive-command-explorer","text":"There is an excellent resource that provides an interactive command explainer that can be accessed at ExplainShell Feel free to use and paste the following commands from the tutorial to see an explanation of the command based on the official commands documentation.","title":"Utilising the ExplainShell interactive command explorer"},{"location":"classes/day-1-reasoning/using-the-command-line/#1-list-files-in-the-current-folder","text":"Directory is used interchangeably with the term folder. ls -l Inside ExplainShell you will see the following explanation:","title":"1. List files in the current folder"},{"location":"classes/day-1-reasoning/using-the-command-line/#2-inspect-your-current-folder-location-in-the-file-system","text":"To see in which folder you are currently in, use the pwd command, short for print working directory . This will return the absolut path to the location you currently stand. NOTE: What is an absolute path? what is a relative path? See this resource for a thorough explanation Again feel free now to copy and paste this command in the ExplainShell pwd","title":"2. Inspect your current folder location in the file system"},{"location":"classes/day-1-reasoning/using-the-command-line/#3-copy-a-file","text":"Now let's go to a terminal window inside of the JupyterLab notebook you have just started up. Select terminal Then you will see the command-line: Before we copy a file, let's use a command-line command echo and create a file with the content \"I am the header of your first README.md\". If you want to know about this command, look it up on ExplainShell . We are also using a command called a redirect -- this will use it to put content into a new file. echo 'I am the header of your first README.md' > README.md You can run ls -l to see that the file is there. As well as see the commands more and another command called less . Use ExplainShell to know about these commands, but they show the contents of the file as you might expect. Now let's create a copy: cp README.md duplicated_README.md Now let's inspect if the file was created, by listing files in the directory with the ls -l command. We are expecting to view the file with name duplicated_README.md in the listed files and folders ls -l","title":"3. Copy a file"},{"location":"classes/day-1-reasoning/using-the-command-line/#4-deleting-a-file","text":"To delete (remove) a file we can use the command rm name_of_folder . Let's delete the duplicate README.md file named duplicated_README.md rm duplicated_README.md Now let's inspect if the file was created, by listing files in the directory with the ls -l command. We are expecting to not view the file with name duplicated_README.md as we just deleted it in the listed files and folders ls -l","title":"4. Deleting a file"},{"location":"classes/day-1-reasoning/using-the-command-line/#5-create-a-new-directory-folder","text":"To create a new folder if that folder doesn't exist, we can use the command mkdir name_of_folder . Let's create a folder named new_folder mkdir new_folder Now let's inspect if the folder was created, by listing files in the directory with the ls -l command. We are expecting to view the folder with name new_folder in the listed files and folders Let's inspect the folder exists with our trusted ls -l command ls -l","title":"5. Create a new directory (folder)"},{"location":"classes/day-1-reasoning/using-the-command-line/#6-delete-a-folder","text":"We can delete a folder using the rm -r command: rm -r new_folder Let's inspect that the folder is indeed deleted: ls -l","title":"6. Delete a folder"},{"location":"classes/day-1-reasoning/using-the-command-line/#7-downloading-a-file-using-a-link","text":"We can retrieve a file using a public link. For example, we have uploaded in a public ZENODO record for this course an example file. The link for the file is: https://zenodo.org/record/4302133/files/deseq2_5k.csv Details of this file, which is a selected subset of the results of running DESeq on RNA-seq files, only those that were with an appropriate p-value (< 0.05) and above the 1.5 fold difference were put into the matrix. https://zenodo.org/record/4302133. You can see the version history, this file was created by a colleague at Lifebit, for the first time this course was offered in 2019 at the Jackson Laboratory. To retrieve this file, we are going to open the terminal window and run the command: cd /sbgenomics/workspace/Kids-First-Elements-of-Style-Workflow-Creation-Maintenance/classes/Running-a-JupyterLab-Notebook wget https://zenodo.org/record/4302133/files/deseq2_5k.csv So please copy this command and paste it in the terminal window and press return - in preparation for the next section of todays class. Reading data and plotting a volcano plot.","title":"7. Downloading a file using a link"},{"location":"classes/day-1-reasoning/using-the-command-line/#return-to-day-1","text":"Return to today's agenda","title":"Return to Day 1"},{"location":"classes/day-2-code-versioning/day-2-code-versioning/","text":"Agenda for the Day 2: Code Versioning Time (UTC) Programme 11.00 - 11.10 Workspace set up and agenda for the day 11.10 - 11.30 Why Git and GitHub? Motivation and set up in the JupyterLab workspace 11.30 - 11.50 GitHub Auth Login: How to authenticate from Jupyter Terminal 12.00 - 12.10 Short break 12.10 - 12.30 Git Routine 1 : Reusing an available repository with fork and how to keep in sync with parent project 12.30 - 12:50: Git Routine 2 : Extend your current code and use Git, GitHub to keep track of changes and contribute 12.50 - 13:00 Wrap up and overview of next days topics Additional resources: GitHub Hello World Happy Git and GitHub for the useR by Jenny Bryan GitHub Documentation","title":"Day 2 - Code versioning"},{"location":"classes/day-2-code-versioning/day-2-code-versioning/#agenda-for-the-day-2-code-versioning","text":"Time (UTC) Programme 11.00 - 11.10 Workspace set up and agenda for the day 11.10 - 11.30 Why Git and GitHub? Motivation and set up in the JupyterLab workspace 11.30 - 11.50 GitHub Auth Login: How to authenticate from Jupyter Terminal 12.00 - 12.10 Short break 12.10 - 12.30 Git Routine 1 : Reusing an available repository with fork and how to keep in sync with parent project 12.30 - 12:50: Git Routine 2 : Extend your current code and use Git, GitHub to keep track of changes and contribute 12.50 - 13:00 Wrap up and overview of next days topics","title":"Agenda for the Day 2: Code Versioning"},{"location":"classes/day-2-code-versioning/day-2-code-versioning/#additional-resources","text":"GitHub Hello World Happy Git and GitHub for the useR by Jenny Bryan GitHub Documentation","title":"Additional resources:"},{"location":"classes/day-2-code-versioning/github-authenticate/","text":"GitHub Authentication To make a contribution to a GitHub repository, you need to authenticate. You can do this with the tool we installed gh . I didn't mention where I found this? Yesterday I told you about Anaconda Package Library . There you can find an active contributor environment and the latest tools that can be used from the command line, in your containers and in your workflows. It is as I mentioned an excellent place to educate and everyday working with these pacakges is easier and easier. Here we see that gh is the GitHub CLI where CLI stands for Command Line Interface , the application we now have been using for the last two days. Generating A GitHub Authentication Token Before we authenticate with GitHub we need to generate a token that we will provide to GitHub to tell that application who we are. This is a Personal Access Token your private secret key to prove who you are. Step 1 - Navigate to Settings, located just under your profile in the upper right hand corner: Step 2 - Navigate to the bottom to < > Developer Settings on the bottom left hand corner: Step 3 - Select Personal access tokens third option from the top on the left side: Step 4 - Select Generate new token on the upper right corner - put a name in the note I used eos Step 5 - Select all of the options and select Generate token Step 6 - Copy the token because as the note mentions - it will not be available again - but you can regenerate tokens now - so don't worry. Now we can authenticate with GitHub Once we have our token, we can now authenticate. Step 1 - Type gh auth login at the command line in the Terminal window Step 2 - Use arrow keeps and select GitHub.com (should be the default) Step 3 - Select HTTPS for your _preferred protocol for git operations Step 4 - Confirm you wish to authenticate with your GitHub Credentials Step 5 - Use arrow keys to select Paste an authentication token Summary of all the steps Return to Agenda","title":"Github authenticate"},{"location":"classes/day-2-code-versioning/github-authenticate/#github-authentication","text":"To make a contribution to a GitHub repository, you need to authenticate. You can do this with the tool we installed gh . I didn't mention where I found this? Yesterday I told you about Anaconda Package Library . There you can find an active contributor environment and the latest tools that can be used from the command line, in your containers and in your workflows. It is as I mentioned an excellent place to educate and everyday working with these pacakges is easier and easier. Here we see that gh is the GitHub CLI where CLI stands for Command Line Interface , the application we now have been using for the last two days.","title":"GitHub Authentication"},{"location":"classes/day-2-code-versioning/github-authenticate/#generating-a-github-authentication-token","text":"Before we authenticate with GitHub we need to generate a token that we will provide to GitHub to tell that application who we are. This is a Personal Access Token your private secret key to prove who you are. Step 1 - Navigate to Settings, located just under your profile in the upper right hand corner: Step 2 - Navigate to the bottom to < > Developer Settings on the bottom left hand corner: Step 3 - Select Personal access tokens third option from the top on the left side: Step 4 - Select Generate new token on the upper right corner - put a name in the note I used eos Step 5 - Select all of the options and select Generate token Step 6 - Copy the token because as the note mentions - it will not be available again - but you can regenerate tokens now - so don't worry.","title":"Generating A GitHub Authentication Token"},{"location":"classes/day-2-code-versioning/github-authenticate/#now-we-can-authenticate-with-github","text":"Once we have our token, we can now authenticate. Step 1 - Type gh auth login at the command line in the Terminal window Step 2 - Use arrow keeps and select GitHub.com (should be the default) Step 3 - Select HTTPS for your _preferred protocol for git operations Step 4 - Confirm you wish to authenticate with your GitHub Credentials Step 5 - Use arrow keys to select Paste an authentication token Summary of all the steps","title":"Now we can authenticate with GitHub"},{"location":"classes/day-2-code-versioning/github-authenticate/#return-to-agenda","text":"","title":"Return to Agenda"},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/","text":"Keeping your Repository Fork in Sync Let's now fork the repository which held the volcano plot yesterday. Fork the https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook repository now. Let's fork the volcano plot repository Step 1 Navigate in your browser window to https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook Step 2 Create a Fork In the browser on the right of the screen you see, Edit Pins Watch and then Fork select Fork Select where you want to fork the repository, your own GitHub repository. You will see the progress window that shows the Forking is in progress Upon success, you will see the following screen Clone your repository exampleVolcanoPlot.png\" These words Clone Fork Pull are the language of Git and GitHub. But they mean as follows: Clone - make a working copy of someone elses repository. Fork - make a copy of someone else's repository, but this one I will make edits to and try to make a contribution. Pull - pull a version of the repository Push - push my changes back to the repository. Synchronizing Your Fork Navigate to your repository within a new Chrome browser tab on GitHub - https://github/[ insert your GitHub user name here ]/exampleVolcanoPlotNotebook. In my case, you will see the NIH-NICHD repository commits behind the adeslatt/exampleVolcanoPlotNotebook So I navigate to the button below code and press the I then get presented the screen: And I press the . And we have synchronize success. But your repositories since they were a fork of the out-of-date repository -- need now to be updated, so let's go ahead and spend a few moments getting your repositories in sync. Your repository is now up-to-date with the master adeslatt/exampleVolcanoPlotNotebook Return to Day 2 Return to Day 2","title":"Keeping your repository fork in sync"},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/#keeping-your-repository-fork-in-sync","text":"Let's now fork the repository which held the volcano plot yesterday. Fork the https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook repository now.","title":"Keeping your Repository Fork in Sync"},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/#lets-fork-the-volcano-plot-repository","text":"Step 1 Navigate in your browser window to https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook Step 2 Create a Fork In the browser on the right of the screen you see, Edit Pins Watch and then Fork select Fork Select where you want to fork the repository, your own GitHub repository. You will see the progress window that shows the Forking is in progress Upon success, you will see the following screen","title":"Let's fork the volcano plot repository"},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/#clone-your-repository-examplevolcanoplotpng","text":"These words Clone Fork Pull are the language of Git and GitHub. But they mean as follows: Clone - make a working copy of someone elses repository. Fork - make a copy of someone else's repository, but this one I will make edits to and try to make a contribution. Pull - pull a version of the repository Push - push my changes back to the repository.","title":"Clone your repository exampleVolcanoPlot.png\""},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/#synchronizing-your-fork","text":"Navigate to your repository within a new Chrome browser tab on GitHub - https://github/[ insert your GitHub user name here ]/exampleVolcanoPlotNotebook. In my case, you will see the NIH-NICHD repository commits behind the adeslatt/exampleVolcanoPlotNotebook So I navigate to the button below code and press the I then get presented the screen: And I press the . And we have synchronize success. But your repositories since they were a fork of the out-of-date repository -- need now to be updated, so let's go ahead and spend a few moments getting your repositories in sync. Your repository is now up-to-date with the master adeslatt/exampleVolcanoPlotNotebook","title":"Synchronizing Your Fork"},{"location":"classes/day-2-code-versioning/keeping-your-repository-fork-in-sync/#return-to-day-2","text":"Return to Day 2","title":"Return to Day 2"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/","text":"Git Routine 2: Extend your current code and use Git, GitHub to Keep track of Changes and collaborate In this exercise, we will use the personal forks as the repository where each of us will contribute. To do that we are going to make a clone of your version of the repository. Use the command line tool mkdir to make a directory with your user name. My user name is adeslatt mkdir adeslatt Move into that directory using the command line tool cd to change directory into your user name cd adeslatt In this case now I am in an empty directory. Now I am going to clone my forked version of the NIH-NICHD exampleVolcanoPlot repository. git clone https://github.com/Wellstein-lab/exampleVolcanoPlotNotebook.git Now we need to move into the exampleVolcanoPlotNotebook directory. cd exampleVolcanoPlotNotebook Type as the following command to see where the upstream location of our file is. A very handy command I use daily git status The git status command helps us inspect to see if there are any changes to our local copy of our repository. git status You should see something like: On branch main Your branch is up to date with 'origin/main'. nothing to commit, working tree clean Edit a file We can edit a file inside your folder README.md Let's use nano or the notebook interface to edit tje README.md file. First lets navigate to the directory for this class. cd <my username>/exampleVolcanoPlotNotebook Next lets edit a file README.md You can use Nano or you can double click the file on the left and add some lines to the readme. Learn about Zenodo here: https://zenodo.org When ready to save the file by using these shortcut keys. in Nano or just hit x and it will ask you to save. CTRL + O This will declare your willingness to save the file. Click ENTER to verify that you want to save the file. Now Close the file and exit from nano with the the shortcut: CTRL + X Awesome, your file has just been saved! You can inspect using the head command, eg head README.md Inspect the changes with git status git status which returns something like On branch main Your branch is up to date with 'origin/main'. Untracked files: (use \"git add <file>...\" to include in what will be committed) README.md nothing added to commit but untracked files present (use \"git add\" to track) We now need to add the modified file to the branch git add README.md We also need to write a message denoting what change was done and finalize the addition. git commit -m \"added useful commands to the README.md\" Now we need to execute the last step -- which is to push the changes to GitHub git push Now we can see what we have done with git status git status And we see we are now up to date. Contributing to the main branch Now we have the changes in our repository. We are out of sync potentially with the main repository. Lets inspect. In the chrome browser, navigate to your repository on GitHub and inspect our local changes. First, we synchronize with the main branch, by pressing the sync button. Next, we make a pull request, this allows us to offer our changes to the main repository. As the repository owner, I can inspect and accept or not accept all the proposed changes. Conclusion of todays class Recap _ You learned about Git and GitHub _ You Fork ed a repository _ You synchronized your Fork to the main repository - You made a contribution with a Pull Request You have entered the world of documenting your work using GitHub -- Congratulations!! Return to Agenda","title":"Git Routine 2:  Extend your current code and use Git, GitHub to Keep track of Changes and collaborate"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#git-routine-2-extend-your-current-code-and-use-git-github-to-keep-track-of-changes-and-collaborate","text":"In this exercise, we will use the personal forks as the repository where each of us will contribute. To do that we are going to make a clone of your version of the repository. Use the command line tool mkdir to make a directory with your user name. My user name is adeslatt mkdir adeslatt Move into that directory using the command line tool cd to change directory into your user name cd adeslatt In this case now I am in an empty directory. Now I am going to clone my forked version of the NIH-NICHD exampleVolcanoPlot repository. git clone https://github.com/Wellstein-lab/exampleVolcanoPlotNotebook.git Now we need to move into the exampleVolcanoPlotNotebook directory. cd exampleVolcanoPlotNotebook Type as the following command to see where the upstream location of our file is.","title":"Git Routine 2:  Extend your current code and use Git, GitHub to Keep track of Changes and collaborate"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#a-very-handy-command-i-use-daily-git-status","text":"The git status command helps us inspect to see if there are any changes to our local copy of our repository. git status You should see something like: On branch main Your branch is up to date with 'origin/main'. nothing to commit, working tree clean","title":"A very handy command I use daily git status"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#edit-a-file","text":"We can edit a file inside your folder README.md Let's use nano or the notebook interface to edit tje README.md file. First lets navigate to the directory for this class. cd <my username>/exampleVolcanoPlotNotebook Next lets edit a file README.md You can use Nano or you can double click the file on the left and add some lines to the readme. Learn about Zenodo here: https://zenodo.org When ready to save the file by using these shortcut keys. in Nano or just hit x and it will ask you to save. CTRL + O This will declare your willingness to save the file. Click ENTER to verify that you want to save the file. Now Close the file and exit from nano with the the shortcut: CTRL + X Awesome, your file has just been saved! You can inspect using the head command, eg head README.md","title":"Edit a file"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#inspect-the-changes-with-git-status","text":"git status which returns something like On branch main Your branch is up to date with 'origin/main'. Untracked files: (use \"git add <file>...\" to include in what will be committed) README.md nothing added to commit but untracked files present (use \"git add\" to track) We now need to add the modified file to the branch git add README.md We also need to write a message denoting what change was done and finalize the addition. git commit -m \"added useful commands to the README.md\" Now we need to execute the last step -- which is to push the changes to GitHub git push Now we can see what we have done with git status git status And we see we are now up to date.","title":"Inspect the changes with git status"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#contributing-to-the-main-branch","text":"Now we have the changes in our repository. We are out of sync potentially with the main repository. Lets inspect. In the chrome browser, navigate to your repository on GitHub and inspect our local changes. First, we synchronize with the main branch, by pressing the sync button. Next, we make a pull request, this allows us to offer our changes to the main repository. As the repository owner, I can inspect and accept or not accept all the proposed changes.","title":"Contributing to the main branch"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#conclusion-of-todays-class","text":"Recap _ You learned about Git and GitHub _ You Fork ed a repository _ You synchronized your Fork to the main repository - You made a contribution with a Pull Request You have entered the world of documenting your work using GitHub -- Congratulations!!","title":"Conclusion of todays class"},{"location":"classes/day-2-code-versioning/the-add-push-git-routine/#return-to-agenda","text":"","title":"Return to Agenda"},{"location":"classes/day-2-code-versioning/why-git-and-setup/","text":"Why Git and GitHub? Motivation and set up in the JupyterLab workspace Why Git? Git is a technology or a system used for version control of code Frees up our code from our local machine (no risk of losing files from your laptop or your jupyterlab notebook on an ephemeral image!) Ensures that we are tracking the changes of our code as a project progresses Allows us to revisit older versions of our work (R codes, notebooks, even this markdown used to make this course - text files, etc). Allows us to keep different versions of our work Why GitHub? Git Hub is a \"code hosting platform for version control and collaboration\" Large community of open source project to browse Enables collaboration of many contributors in the same project Serves as a code vault for making our reasearch code findable and our methods transparent Setting up in the terminal In the JupyterLab notebook we started on Cavatica, looking like this: Please open the Terminal window. We are going back to the command line. in zoom to know all is well. Using the nano file editor Before we start working with Git in the command line, we will learn one more helpful skill. How to use a code editor. A code editors allows us to edit files in an environment without a graphical user interface. For us, it will be needed in some Git related tasks, so we want to be prepared. Edit a file using nano in only 4 steps: Your screen likely looks similar to this: 1. Type nano hello.txt This will create and open a new file hello.txt . If the file already exists, it will open that existing file. If the file is not yet in the directory, the screen looks like this: 2. Write inside the open file named hello.txt As you typically would type in any Document editor (eg. Word, Google Docs, etc). You can type for example, Hello Git! 3. When ready to save type the shortcut: CTRL + O This will declare your willingness to save the file. Click ENTER to verify that you want to save the file. Now you will see it in the directory on the left as well showing that the file has now been saved. 4. Close the file and exit from nano with the the shortcut: CTRL + X Awesome, your file has just been saved! You can inspect using another command line function, head . In the terminal window at the command line, type: head hello.txt We will probably need soon nano in one of our Git tasks. Creating an environment to install packages Not exactly required as the base environment within CAVATICA JupyterLab is already there. But as a habit I begin with creating an conda environment so that I can keep track of what I add. When we get to containerizing, you will see how using conda will help keep all of the details about what you need to install safely as minimal as you can. We will cover that more on the containerization less, which is tomorrow. conda create -n eos -y Once the environment is created, you activate it as instructed conda activate eos Then I install two additional things that I use on a regular basis, GitHub CLI and emacs conda install -c conda-forge gh -y (Optionally install emacs) conda install -c conda-forge emacs -y More on Anaconda and packages later. Configuring our GitHub user information To be able to use Git and GitHub from the command line we need to configure the information related to our GitHub user. Let's follow the commands below to set in the workspace the required information. Set user name Replace <my github user name> with your actual GitHub email. git config --global user.name <my github user name> Set user email Replace <my email associated with my github user name> with your actual GitHub email. git config --global user.email <my email associated with my github user name> (Optional) Set preferred file editor eg nano or emacs git config --global core.editor nano Or if you like emacs , or vim , change it to your desired editor . git config --global core.editor emacs Return to Agenda","title":"Why Git and GitHub?"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#why-git-and-github","text":"Motivation and set up in the JupyterLab workspace","title":"Why Git and GitHub?"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#why-git","text":"Git is a technology or a system used for version control of code Frees up our code from our local machine (no risk of losing files from your laptop or your jupyterlab notebook on an ephemeral image!) Ensures that we are tracking the changes of our code as a project progresses Allows us to revisit older versions of our work (R codes, notebooks, even this markdown used to make this course - text files, etc). Allows us to keep different versions of our work","title":"Why Git?"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#why-github","text":"Git Hub is a \"code hosting platform for version control and collaboration\" Large community of open source project to browse Enables collaboration of many contributors in the same project Serves as a code vault for making our reasearch code findable and our methods transparent","title":"Why GitHub?"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#setting-up-in-the-terminal","text":"In the JupyterLab notebook we started on Cavatica, looking like this: Please open the Terminal window. We are going back to the command line. in zoom to know all is well.","title":"Setting up in the terminal"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#using-the-nano-file-editor","text":"Before we start working with Git in the command line, we will learn one more helpful skill. How to use a code editor. A code editors allows us to edit files in an environment without a graphical user interface. For us, it will be needed in some Git related tasks, so we want to be prepared. Edit a file using nano in only 4 steps: Your screen likely looks similar to this:","title":"Using the nano file editor"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#1-type-nano-hellotxt","text":"This will create and open a new file hello.txt . If the file already exists, it will open that existing file. If the file is not yet in the directory, the screen looks like this:","title":"1. Type nano hello.txt"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#2-write-inside-the-open-file-named-hellotxt","text":"As you typically would type in any Document editor (eg. Word, Google Docs, etc). You can type for example, Hello Git!","title":"2. Write inside the open file named hello.txt"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#3-when-ready-to-save-type-the-shortcut","text":"CTRL + O This will declare your willingness to save the file. Click ENTER to verify that you want to save the file. Now you will see it in the directory on the left as well showing that the file has now been saved.","title":"3. When ready to save type the shortcut:"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#4-close-the-file-and-exit-from-nano-with-the-the-shortcut","text":"CTRL + X Awesome, your file has just been saved! You can inspect using another command line function, head . In the terminal window at the command line, type: head hello.txt We will probably need soon nano in one of our Git tasks.","title":"4. Close the file and exit from nano with the the shortcut:"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#creating-an-environment-to-install-packages","text":"Not exactly required as the base environment within CAVATICA JupyterLab is already there. But as a habit I begin with creating an conda environment so that I can keep track of what I add. When we get to containerizing, you will see how using conda will help keep all of the details about what you need to install safely as minimal as you can. We will cover that more on the containerization less, which is tomorrow. conda create -n eos -y Once the environment is created, you activate it as instructed conda activate eos Then I install two additional things that I use on a regular basis, GitHub CLI and emacs conda install -c conda-forge gh -y","title":"Creating an environment to install packages"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#optionally-install-emacs","text":"conda install -c conda-forge emacs -y More on Anaconda and packages later.","title":"(Optionally install emacs)"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#configuring-our-github-user-information","text":"To be able to use Git and GitHub from the command line we need to configure the information related to our GitHub user. Let's follow the commands below to set in the workspace the required information.","title":"Configuring our GitHub user information"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#set-user-name","text":"Replace <my github user name> with your actual GitHub email. git config --global user.name <my github user name>","title":"Set user name"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#set-user-email","text":"Replace <my email associated with my github user name> with your actual GitHub email. git config --global user.email <my email associated with my github user name>","title":"Set user email"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#optional-set-preferred-file-editor-eg-nano-or-emacs","text":"git config --global core.editor nano Or if you like emacs , or vim , change it to your desired editor . git config --global core.editor emacs","title":"(Optional) Set preferred file editor eg nano or emacs"},{"location":"classes/day-2-code-versioning/why-git-and-setup/#return-to-agenda","text":"","title":"Return to Agenda"},{"location":"classes/day-2-code-versioning/workspace-setup/","text":"Workspace Set-up Today, we will start either with the JupyterLab Notebook you started yesterday or if you have not already registered and started a JupyterLab Notebook, please go ahead and do so. For the class, we recommend you use the Chrome Browswer. And as you click through the lesson, open each link in a new tab. Already had a notebook yesterday Navigate to CAVATICA and go ahead and log in . You will land at a Dashboard, on the left you have your projects, and on your right your analyses. Select Data Studio You will see all your past analyses. Note that you do not pay for your analyses when they are stopped. You also see documentation regarding the environment you had set up, the cost and the duration spent on the session. You will also see Files and Settings . You Do pay for storage. The Cloud Cost Overview the Kids First DRC helps you calculate costs. Files Lets look at Files. In general, adopting a habit of getting in to do your work, developing your analysis and your workflows in small pieces with small files before you execute over 100s if not 1000s of files, will save you time and money. Be judicious in what files you need to keep. This is a habit that will save you. This is why we use GitHub, we deposit our original measurement data in appropriate public storage sites. We use repositories such as Zenodo for Data Object Identifiers and GitHub to track our Notebooks, which are essentially our scientific electronic notebooks, and our workflows. Settings Here you have the opportunity to change the size of your machine. If you scroll down, you will see the size and the price of the machine. One thing to note. We are using a dedicated instance when we are doing interactive analysis. That is why the price is $0.34/hour. When we run a workflow, we typically use spot instances that are usually 1/8th the cost or so. This then allows us to Fire and Forget . The beauty of spot instances is not only are they cheaper, but they help enforce that they are ephemeral, that is from a security, cost standpoint they are ideal. They exist, their images loaded onto them and then they are gone. This is why we always run workflows that are in GitHub and/or an App that is persistent. The memory of what was run is held in the repository. So when you publish, or you collaborate, or when you ship your analyses or your workflow to a collaborator, you can share the repository, making configuration adjustments for platform differences within the repository. More on that later. Start your notebook. Go ahead and hit and lets get started. Give us a in the Zoom chat if all is going ok. You will soon see the JupyterLab Launch pad - which as we know will stay alive for about 30 minutes. Lets go back to the lesson and understand a bit about Why Git and GitHub . Starting From Scratch If you were not with us yesterday, please follow these directions to start a notebook. If you need help, we will help you at the coffee break or reach out to David in the Chat. Continue with our Lesson for Day 2 Return to Agenda","title":"Workspace Set-up"},{"location":"classes/day-2-code-versioning/workspace-setup/#workspace-set-up","text":"Today, we will start either with the JupyterLab Notebook you started yesterday or if you have not already registered and started a JupyterLab Notebook, please go ahead and do so. For the class, we recommend you use the Chrome Browswer. And as you click through the lesson, open each link in a new tab.","title":"Workspace Set-up"},{"location":"classes/day-2-code-versioning/workspace-setup/#already-had-a-notebook-yesterday","text":"Navigate to CAVATICA and go ahead and log in . You will land at a Dashboard, on the left you have your projects, and on your right your analyses.","title":"Already had a notebook yesterday"},{"location":"classes/day-2-code-versioning/workspace-setup/#select-data-studio","text":"You will see all your past analyses. Note that you do not pay for your analyses when they are stopped. You also see documentation regarding the environment you had set up, the cost and the duration spent on the session. You will also see Files and Settings . You Do pay for storage. The Cloud Cost Overview the Kids First DRC helps you calculate costs.","title":"Select Data Studio"},{"location":"classes/day-2-code-versioning/workspace-setup/#files","text":"Lets look at Files. In general, adopting a habit of getting in to do your work, developing your analysis and your workflows in small pieces with small files before you execute over 100s if not 1000s of files, will save you time and money. Be judicious in what files you need to keep. This is a habit that will save you. This is why we use GitHub, we deposit our original measurement data in appropriate public storage sites. We use repositories such as Zenodo for Data Object Identifiers and GitHub to track our Notebooks, which are essentially our scientific electronic notebooks, and our workflows.","title":"Files"},{"location":"classes/day-2-code-versioning/workspace-setup/#settings","text":"Here you have the opportunity to change the size of your machine. If you scroll down, you will see the size and the price of the machine. One thing to note. We are using a dedicated instance when we are doing interactive analysis. That is why the price is $0.34/hour. When we run a workflow, we typically use spot instances that are usually 1/8th the cost or so. This then allows us to Fire and Forget . The beauty of spot instances is not only are they cheaper, but they help enforce that they are ephemeral, that is from a security, cost standpoint they are ideal. They exist, their images loaded onto them and then they are gone. This is why we always run workflows that are in GitHub and/or an App that is persistent. The memory of what was run is held in the repository. So when you publish, or you collaborate, or when you ship your analyses or your workflow to a collaborator, you can share the repository, making configuration adjustments for platform differences within the repository. More on that later.","title":"Settings"},{"location":"classes/day-2-code-versioning/workspace-setup/#start-your-notebook","text":"Go ahead and hit and lets get started. Give us a in the Zoom chat if all is going ok. You will soon see the JupyterLab Launch pad - which as we know will stay alive for about 30 minutes. Lets go back to the lesson and understand a bit about Why Git and GitHub .","title":"Start your notebook."},{"location":"classes/day-2-code-versioning/workspace-setup/#starting-from-scratch","text":"If you were not with us yesterday, please follow these directions to start a notebook. If you need help, we will help you at the coffee break or reach out to David in the Chat.","title":"Starting From Scratch"},{"location":"classes/day-2-code-versioning/workspace-setup/#continue-with-our-lesson-for-day-2","text":"Return to Agenda","title":"Continue with our Lesson for Day 2"},{"location":"classes/day-3-containerization/building-dockerfiles/","text":"Building Dockerfiles We have now learned that we can control our environment with conda . If you haven't done that lesson, it would be best for you to go through that lesson before beginning with this step. The Conda Lesson will show you how to create a clean environment. Now we will show how to build an Docker image Docker from a Dockerfile and we will use this in our workflow in three ways: in a Nextflow workflow script, in a common workflow script and as an Application within CAVATICA. What is a Docker image and a Dockerfile Docker images are created by using a so called Dockerfile i.e. a simple text file containing a list of commands to be executed to assemble and configure the image with the software packages required. In this step, you will create a Docker image containing the FastQC & MultiQC tools. Warning: the Docker build process automatically copies all files that are located in the current directory to the Docker daemon in order to create the image. This can take a lot of time when big/many files exist. For this reason, it's important to always work in a directory containing only the files you really need to include in your Docker image. Alternatively, you can use the .dockerignore file to select the path to exclude from the build. Navigate to the Google Shell Window We will use both the editor and the shell terminal window to accomplish these next tasks. If you do not already have this activated, activate your environment we just made. Copy the lines below or type in your own terminal window. conda activate eos Creating your first Dockerfile Then make the clean directory for us to work in. Copy the lines here below into your own window. mkdir fastqc-docker Now navigate into that directory, again you can type this or simply copy the lines below in your google shell terminal window. cd fastqc-docker Now we are going to create our first Dockerfile touch is another handy bash command for creating an empty file. touch Dockerfile The docker application requires the exact filename Dockerfile when creating the image. In the google shell editor window this time, open the file named Dockerfile Copy the contents of the window below and paste it into your window. # Full contents of Dockerfile FROM continuumio/miniconda3 LABEL description=\"Base docker image with conda and util libraries\" ARG ENV_NAME=\"fastqc\" # Install the conda environment COPY environment.yml / RUN conda env create --quiet --name ${ENV_NAME} --file /environment.yml && conda clean -a # Add conda installation dir to PATH (instead of doing 'conda activate') ENV PATH /opt/conda/envs/${ENV_NAME}/bin:$PATH After pasting in the window, your file should look like this: The docker application understands the commands FROM , LABEL , ARG ENV_NAME , COPY , RUN and ENV PATH . It also knows that # are comments and are for the human reader that is reading this file to understand what is happening within the file itself. The base image we are using for the Dockerfile is from continuumio the maker of our miniconda3 which we installed earlier. It is because of that environment that we do not need to install conda , it is part of the image. We are using the function COPY to bring it into the image. We are adding to this base image our new function fastqc . You will see later that you can COPY your own custom scripts for use within the container. This is the process of containerization . Notice as well we are prompted to look at our quotas, not an infinite resource but pretty good availability for a free asset. Now we will inspect our environment.yml which is being used by conda to install our desired application. Creating your environment.yml File Notice that this Docker file uses a file called environment.yml . Back in the terminal window again: Type at the prompt touch environment.yml Return to the Google Shell editor and open the file named environment.yml and copy the contents of the window below into that file. What we are doing is using conda to control our environment. And this file is used within our Dockerfile to create the controlled environment and install the function, fastqc . The channels listed are exactly the same channels we would have used if we wanted to make the fastqc application available within our environment to use on the command line. What we are doing is then reproducing this command line environment within our soon to be Docker image. name: fastqc channels: - bioconda - defaults dependencies: - fastqc Building our Dockerfile image for fastqc Build the Docker image by using the following command: There are three steps with building a Docker image: build tag push Here below we are executing the build command with a tag, indicated by the -t . The . says to build in the current directory. This is the normal way to build your Docker image because as we noted before, the docker application will use all that is within your current directory to make its image. docker build -t fastqc . Here the google shell asks to authorize it to complete the task. When it completes, we can verify that the image has been created listing all available images with the following command: docker images Test this image from the command line By now you see what we have done. You could install fastqc using the command conda install -c bioconda fastqc -y We see what is happening is using the Anaconda channel bioconda we are installing fastqc for our local use within our own file system on this small virtual instance made free for us from Google . Let's do that real quick to show what I mean. Go ahead and copy the line above on the command line and install fastqc within our eos environment. As expected it has been installed within the binary directory, the directory that contains executables and shortened to the unix convention to be just bin of our controlled environment eos . This would be fine if we wanted to do local work on some fastqc files. But it would not be available for the workflow that will be spun up on ephemeral machines where we would have to build our entire environment from scratch. But I have made the argument, that what is more sustainable is if we containerize at the process level, our work will be more durable, more portable, and more repurposable. That is it will be available for me to use for future work. So we built a container image. Now how could we use that container image? To test this tool from the command line Set up an environment variable capturing your current command line: PWD=$(pwd) To make our local directory available to the image, which is a self-contained environment, we need to do something called mounting and use your current directory and call the tool now encapsulated within the environment. I use this command to do that. docker run -it -v $PWD:$PWD -w $PWD fastqc fastqc -h You can read more about what is happening in detail with Docker containers and images . You don't need to know every detail to use them. Add this code to your GitHub repository It is a best practice with GitHub to always add a README.md . Let's add this file and then from the command line use the GitHub command line tools to push this to our GitHub repository. touch README.md Open with the Code Editor (the Markdown Editor is experimental). To do so, hover over the README.md file and click Open With... and select Code Editor . Copy the contents here that explain what we have in this repository # fastqc-docker Build a Container for fastqc from bioconda Steps to build this docker container. 1. Look up on [anaconda](https://anaconda.org/) the tool you wish to install 2. create an `environment.yml` file either manually or automatically 3. Use the template `Dockerfile` modifying if necessary (in our case we have no custom files for the `src` directory so we do not use that) 4. Build the Docker Image 5. Set up GitHub Actions To build your image from the command line: * Can do this on [Google shell](https://shell.cloud.google.com) - docker is installed and available \\```bash docker build -t fastqc . \\``` To test this tool from the command line Set up an environment variable capturing your current command line: \\```bash PWD=$(pwd) \\``` Then mount and use your current directory and call the tool now encapsulated within the environment. \\```bash docker run -it -v $PWD:$PWD -w $PWD fastqc fastqc -h \\``` Now edit the file (I added the \\ so Markdown would ignore the data but it means we need to edit them out for the README.md to properly render on the repo. Please remove all the \\ 's the file will look like this: Use GitHub Command line tool gh to authenticate before we push create our repository. The Google shell comes with the gh already installed. So we just begin with authentication. gh auth login As we did yesterday, authenticate with your Personal Authentication Token (eos) ad376@cloudshell:~/fastqc-docker$ gh auth login ? What account do you want to log into? GitHub.com ? What is your preferred protocol for Git operations? HTTPS ? Authenticate Git with your GitHub credentials? Yes ? How would you like to authenticate GitHub CLI? [Use arrows to move, type to filter] Login with a web browser > Paste an authentication token Now we can from the command line create this repository. Let's go ahead and tell git who we are. git config --global user.email \"adeslat@scitechcon.org\" git config --global user.name \"adeslatt\" Following GitHub 's updated instructions on how to create a new repository from the command line We now type git init -b main Which will return Initialized empty Git repository in /home/ad376/fastqc-docker/.git/ And then we type git add . && git commit -m \"initial commit\" which returns something like this. [main (root-commit) d1e421f] initial commit 3 files changed, 33 insertions(+) create mode 100644 Dockerfile create mode 100644 README.md create mode 100644 environment.yml We then use the gh repo create command to create the reposistory. gh repo create Which then prompts us to what we need to do -- important is that what we want to do is Push an existing local repository to GitHub . The tool's remainder defaults are acceptable, because we staged ourselves with the name of the directory. (eos) ad376@cloudshell:~/fastqc-docker$ gh repo create ? What would you like to do? Push an existing local repository to GitHub ? Path to local repository . ? Repository name fastqc-docker ? Description a container for the bioconda command fastqc ? Visibility Public \u2713 Created repository adeslatt/fastqc-docker on GitHub ? Add a remote? Yes ? What should the new remote be called? origin \u2713 Added remote https://github.com/adeslatt/fastqc-docker.git ? Would you like to push commits from the current branch to \"origin\"? Yes \u2713 Pushed commits to https://github.com/adeslatt/fastqc-docker.git Building the multiqc Docker image Navigate to the top of your home directory cd ~ Clone the multiqc-docker repository. git clone https://github.com/adeslatt/multiqc-docker.git Creating Your Second Dockerfile Navigate now to your home directory on the google shell. Remember that .. is the parent directory. cd .. mkdir multiqc-docker Now navigate into that directory, again you can type this or simply copy the lines below in your google shell terminal window. cd multiqc-docker Now we are going to create our first Dockerfile touch is another handy bash command for creating an empty file. touch Dockerfile The docker application requires the exact filename Dockerfile when creating the image. Again in the google shell editor window open the file named Dockerfile Copy the contents of the window below and paste it into your window. # Full contents of Dockerfile FROM continuumio/miniconda3 LABEL description=\"Base docker image with conda and util libraries\" ARG ENV_NAME=\"multiqc\" # Install the conda environment COPY environment.yml / RUN conda env create --quiet --name ${ENV_NAME} --file /environment.yml && conda clean -a # Add conda installation dir to PATH (instead of doing 'conda activate') ENV PATH /opt/conda/envs/${ENV_NAME}/bin:$PATH Creating your multiqc environment.yml File Back in the terminal window again: Type at the prompt touch environment.yml What we are doing is then reproducing this command line environment within our soon to be Docker image. name: multiqc channels: - bioconda - defaults dependencies: - multiqc And build the image docker build -t multiqc . Inspect what images you have now available to you locally You can see what you have built -- and see that we have tag ged our files in a certain way docker images Add this multiqc code to your GitHub repository It is a best practice with GitHub to always add a README.md . Let's add this file and then from the command line use the GitHub command line tools to push this to our GitHub repository. touch README.md Open with the Code Editor (the Markdown Editor is experimental). To do so, hover over the README.md file and click Open With... and select Code Editor . # multiqc-docker Build a Container for multiqc from bioconda Steps to build this docker container. 1. Look up on [anaconda](https://anaconda.org/) the tool you wish to install 2. create an `environment.yml` file either manually or automatically 3. Use the template `Dockerfile` modifying if necessary (in our case we have no custom files for the `src` directory so we do not use that) 4. Build the Docker Image 5. Set up GitHub Actions To build your image from the command line: * Can do this on [Google shell](https://shell.cloud.google.com) - docker is installed and available \\```bash docker build -t multiqc . \\``` To test this tool from the command line Set up an environment variable capturing your current command line: \\```bash PWD=$(pwd) \\``` Then mount and use your current directory and call the tool now encapsulated within the environment. \\```bash docker run -it -v $PWD:$PWD -w $PWD multiqc multiqc -h \\``` Now edit the file (I added the \\ so Markdown would ignore the data but it means we need to edit them out for the README.md to properly render on the repo. Please remove all the \\ 's the file will look like this: After pasting in the window, your file should look like this: We now type git init -b main Which will return Initialized empty Git repository in /home/ad376/fastqc-docker/.git/ And then we type git add . && git commit -m \"initial commit\" which returns something like this. [main (root-commit) d1e421f] initial commit 3 files changed, 33 insertions(+) create mode 100644 Dockerfile create mode 100644 README.md create mode 100644 environment.yml We then use the gh repo create command to create the reposistory. gh repo create Which then prompts us to what we need to do -- important is that what we want to do is Push an existing local repository to GitHub . The tool's remainder defaults are acceptable, because we staged ourselves with the name of the directory. (eos) ad376@cloudshell:~/multiqc-docker$ gh repo create ? What would you like to do? Push an existing local repository to GitHub ? Path to local repository . ? Repository name multiqc-docker ? Description a container for the bioconda command multiqc ? Visibility Public \u2713 Created repository adeslatt/fastqc-docker on GitHub ? Add a remote? Yes ? What should the new remote be called? origin \u2713 Added remote https://github.com/adeslatt/multiqc-docker.git ? Would you like to push commits from the current branch to \"origin\"? Yes \u2713 Pushed commits to https://github.com/adeslatt/multiqc-docker.git Upon success your repository will be in place on your GitHub. Recap We have created a Dockerfile using the Google Shell Editor We have created the environment.yml file also with the Google Shell Editor We have used the new command placed our desired function fastqc within the environment.yml file for the ultimate Docker container. We built the container for the application fastqc in the fastqc-docker subdirectory. We used GitHub command line tool gh to take our local repositories and put both up on GitHub. We inspected our images. Tomorrow, we will make our first workflow and show how to use these containers within our private repository on CAVATICA Return to the Agenda","title":"Building dockerfiles"},{"location":"classes/day-3-containerization/building-dockerfiles/#building-dockerfiles","text":"We have now learned that we can control our environment with conda . If you haven't done that lesson, it would be best for you to go through that lesson before beginning with this step. The Conda Lesson will show you how to create a clean environment. Now we will show how to build an Docker image Docker from a Dockerfile and we will use this in our workflow in three ways: in a Nextflow workflow script, in a common workflow script and as an Application within CAVATICA.","title":"Building Dockerfiles"},{"location":"classes/day-3-containerization/building-dockerfiles/#what-is-a-docker-image-and-a-dockerfile","text":"Docker images are created by using a so called Dockerfile i.e. a simple text file containing a list of commands to be executed to assemble and configure the image with the software packages required. In this step, you will create a Docker image containing the FastQC & MultiQC tools. Warning: the Docker build process automatically copies all files that are located in the current directory to the Docker daemon in order to create the image. This can take a lot of time when big/many files exist. For this reason, it's important to always work in a directory containing only the files you really need to include in your Docker image. Alternatively, you can use the .dockerignore file to select the path to exclude from the build.","title":"What is a Docker image and a Dockerfile"},{"location":"classes/day-3-containerization/building-dockerfiles/#navigate-to-the-google-shell-window","text":"We will use both the editor and the shell terminal window to accomplish these next tasks. If you do not already have this activated, activate your environment we just made. Copy the lines below or type in your own terminal window. conda activate eos","title":"Navigate to the Google Shell Window"},{"location":"classes/day-3-containerization/building-dockerfiles/#creating-your-first-dockerfile","text":"Then make the clean directory for us to work in. Copy the lines here below into your own window. mkdir fastqc-docker Now navigate into that directory, again you can type this or simply copy the lines below in your google shell terminal window. cd fastqc-docker Now we are going to create our first Dockerfile touch is another handy bash command for creating an empty file. touch Dockerfile The docker application requires the exact filename Dockerfile when creating the image. In the google shell editor window this time, open the file named Dockerfile Copy the contents of the window below and paste it into your window. # Full contents of Dockerfile FROM continuumio/miniconda3 LABEL description=\"Base docker image with conda and util libraries\" ARG ENV_NAME=\"fastqc\" # Install the conda environment COPY environment.yml / RUN conda env create --quiet --name ${ENV_NAME} --file /environment.yml && conda clean -a # Add conda installation dir to PATH (instead of doing 'conda activate') ENV PATH /opt/conda/envs/${ENV_NAME}/bin:$PATH After pasting in the window, your file should look like this: The docker application understands the commands FROM , LABEL , ARG ENV_NAME , COPY , RUN and ENV PATH . It also knows that # are comments and are for the human reader that is reading this file to understand what is happening within the file itself. The base image we are using for the Dockerfile is from continuumio the maker of our miniconda3 which we installed earlier. It is because of that environment that we do not need to install conda , it is part of the image. We are using the function COPY to bring it into the image. We are adding to this base image our new function fastqc . You will see later that you can COPY your own custom scripts for use within the container. This is the process of containerization . Notice as well we are prompted to look at our quotas, not an infinite resource but pretty good availability for a free asset. Now we will inspect our environment.yml which is being used by conda to install our desired application.","title":"Creating your first Dockerfile"},{"location":"classes/day-3-containerization/building-dockerfiles/#creating-your-environmentyml-file","text":"Notice that this Docker file uses a file called environment.yml . Back in the terminal window again: Type at the prompt touch environment.yml Return to the Google Shell editor and open the file named environment.yml and copy the contents of the window below into that file. What we are doing is using conda to control our environment. And this file is used within our Dockerfile to create the controlled environment and install the function, fastqc . The channels listed are exactly the same channels we would have used if we wanted to make the fastqc application available within our environment to use on the command line. What we are doing is then reproducing this command line environment within our soon to be Docker image. name: fastqc channels: - bioconda - defaults dependencies: - fastqc","title":"Creating your environment.yml File"},{"location":"classes/day-3-containerization/building-dockerfiles/#building-our-dockerfile-image-for-fastqc","text":"Build the Docker image by using the following command: There are three steps with building a Docker image: build tag push Here below we are executing the build command with a tag, indicated by the -t . The . says to build in the current directory. This is the normal way to build your Docker image because as we noted before, the docker application will use all that is within your current directory to make its image. docker build -t fastqc . Here the google shell asks to authorize it to complete the task. When it completes, we can verify that the image has been created listing all available images with the following command: docker images","title":"Building our Dockerfile image for fastqc"},{"location":"classes/day-3-containerization/building-dockerfiles/#test-this-image-from-the-command-line","text":"By now you see what we have done. You could install fastqc using the command conda install -c bioconda fastqc -y We see what is happening is using the Anaconda channel bioconda we are installing fastqc for our local use within our own file system on this small virtual instance made free for us from Google . Let's do that real quick to show what I mean. Go ahead and copy the line above on the command line and install fastqc within our eos environment. As expected it has been installed within the binary directory, the directory that contains executables and shortened to the unix convention to be just bin of our controlled environment eos . This would be fine if we wanted to do local work on some fastqc files. But it would not be available for the workflow that will be spun up on ephemeral machines where we would have to build our entire environment from scratch. But I have made the argument, that what is more sustainable is if we containerize at the process level, our work will be more durable, more portable, and more repurposable. That is it will be available for me to use for future work. So we built a container image. Now how could we use that container image? To test this tool from the command line Set up an environment variable capturing your current command line: PWD=$(pwd) To make our local directory available to the image, which is a self-contained environment, we need to do something called mounting and use your current directory and call the tool now encapsulated within the environment. I use this command to do that. docker run -it -v $PWD:$PWD -w $PWD fastqc fastqc -h You can read more about what is happening in detail with Docker containers and images . You don't need to know every detail to use them.","title":"Test this image from the command line"},{"location":"classes/day-3-containerization/building-dockerfiles/#add-this-code-to-your-github-repository","text":"It is a best practice with GitHub to always add a README.md . Let's add this file and then from the command line use the GitHub command line tools to push this to our GitHub repository. touch README.md Open with the Code Editor (the Markdown Editor is experimental). To do so, hover over the README.md file and click Open With... and select Code Editor . Copy the contents here that explain what we have in this repository # fastqc-docker Build a Container for fastqc from bioconda Steps to build this docker container. 1. Look up on [anaconda](https://anaconda.org/) the tool you wish to install 2. create an `environment.yml` file either manually or automatically 3. Use the template `Dockerfile` modifying if necessary (in our case we have no custom files for the `src` directory so we do not use that) 4. Build the Docker Image 5. Set up GitHub Actions To build your image from the command line: * Can do this on [Google shell](https://shell.cloud.google.com) - docker is installed and available \\```bash docker build -t fastqc . \\``` To test this tool from the command line Set up an environment variable capturing your current command line: \\```bash PWD=$(pwd) \\``` Then mount and use your current directory and call the tool now encapsulated within the environment. \\```bash docker run -it -v $PWD:$PWD -w $PWD fastqc fastqc -h \\``` Now edit the file (I added the \\ so Markdown would ignore the data but it means we need to edit them out for the README.md to properly render on the repo. Please remove all the \\ 's the file will look like this:","title":"Add this code to your GitHub repository"},{"location":"classes/day-3-containerization/building-dockerfiles/#use-github-command-line-tool-gh-to-authenticate-before-we-push-create-our-repository","text":"The Google shell comes with the gh already installed. So we just begin with authentication. gh auth login As we did yesterday, authenticate with your Personal Authentication Token (eos) ad376@cloudshell:~/fastqc-docker$ gh auth login ? What account do you want to log into? GitHub.com ? What is your preferred protocol for Git operations? HTTPS ? Authenticate Git with your GitHub credentials? Yes ? How would you like to authenticate GitHub CLI? [Use arrows to move, type to filter] Login with a web browser > Paste an authentication token Now we can from the command line create this repository. Let's go ahead and tell git who we are. git config --global user.email \"adeslat@scitechcon.org\" git config --global user.name \"adeslatt\" Following GitHub 's updated instructions on how to create a new repository from the command line We now type git init -b main Which will return Initialized empty Git repository in /home/ad376/fastqc-docker/.git/ And then we type git add . && git commit -m \"initial commit\" which returns something like this. [main (root-commit) d1e421f] initial commit 3 files changed, 33 insertions(+) create mode 100644 Dockerfile create mode 100644 README.md create mode 100644 environment.yml We then use the gh repo create command to create the reposistory. gh repo create Which then prompts us to what we need to do -- important is that what we want to do is Push an existing local repository to GitHub . The tool's remainder defaults are acceptable, because we staged ourselves with the name of the directory. (eos) ad376@cloudshell:~/fastqc-docker$ gh repo create ? What would you like to do? Push an existing local repository to GitHub ? Path to local repository . ? Repository name fastqc-docker ? Description a container for the bioconda command fastqc ? Visibility Public \u2713 Created repository adeslatt/fastqc-docker on GitHub ? Add a remote? Yes ? What should the new remote be called? origin \u2713 Added remote https://github.com/adeslatt/fastqc-docker.git ? Would you like to push commits from the current branch to \"origin\"? Yes \u2713 Pushed commits to https://github.com/adeslatt/fastqc-docker.git","title":"Use GitHub Command line tool gh to authenticate before we push create our repository."},{"location":"classes/day-3-containerization/building-dockerfiles/#building-the-multiqc-docker-image","text":"Navigate to the top of your home directory cd ~ Clone the multiqc-docker repository. git clone https://github.com/adeslatt/multiqc-docker.git","title":"Building the multiqc Docker image"},{"location":"classes/day-3-containerization/building-dockerfiles/#creating-your-second-dockerfile","text":"Navigate now to your home directory on the google shell. Remember that .. is the parent directory. cd .. mkdir multiqc-docker Now navigate into that directory, again you can type this or simply copy the lines below in your google shell terminal window. cd multiqc-docker Now we are going to create our first Dockerfile touch is another handy bash command for creating an empty file. touch Dockerfile The docker application requires the exact filename Dockerfile when creating the image. Again in the google shell editor window open the file named Dockerfile Copy the contents of the window below and paste it into your window. # Full contents of Dockerfile FROM continuumio/miniconda3 LABEL description=\"Base docker image with conda and util libraries\" ARG ENV_NAME=\"multiqc\" # Install the conda environment COPY environment.yml / RUN conda env create --quiet --name ${ENV_NAME} --file /environment.yml && conda clean -a # Add conda installation dir to PATH (instead of doing 'conda activate') ENV PATH /opt/conda/envs/${ENV_NAME}/bin:$PATH","title":"Creating Your Second Dockerfile"},{"location":"classes/day-3-containerization/building-dockerfiles/#creating-your-multiqc-environmentyml-file","text":"Back in the terminal window again: Type at the prompt touch environment.yml What we are doing is then reproducing this command line environment within our soon to be Docker image. name: multiqc channels: - bioconda - defaults dependencies: - multiqc And build the image docker build -t multiqc .","title":"Creating your multiqc environment.yml File"},{"location":"classes/day-3-containerization/building-dockerfiles/#inspect-what-images-you-have-now-available-to-you-locally","text":"You can see what you have built -- and see that we have tag ged our files in a certain way docker images","title":"Inspect what images you have now available to you locally"},{"location":"classes/day-3-containerization/building-dockerfiles/#add-this-multiqc-code-to-your-github-repository","text":"It is a best practice with GitHub to always add a README.md . Let's add this file and then from the command line use the GitHub command line tools to push this to our GitHub repository. touch README.md Open with the Code Editor (the Markdown Editor is experimental). To do so, hover over the README.md file and click Open With... and select Code Editor . # multiqc-docker Build a Container for multiqc from bioconda Steps to build this docker container. 1. Look up on [anaconda](https://anaconda.org/) the tool you wish to install 2. create an `environment.yml` file either manually or automatically 3. Use the template `Dockerfile` modifying if necessary (in our case we have no custom files for the `src` directory so we do not use that) 4. Build the Docker Image 5. Set up GitHub Actions To build your image from the command line: * Can do this on [Google shell](https://shell.cloud.google.com) - docker is installed and available \\```bash docker build -t multiqc . \\``` To test this tool from the command line Set up an environment variable capturing your current command line: \\```bash PWD=$(pwd) \\``` Then mount and use your current directory and call the tool now encapsulated within the environment. \\```bash docker run -it -v $PWD:$PWD -w $PWD multiqc multiqc -h \\``` Now edit the file (I added the \\ so Markdown would ignore the data but it means we need to edit them out for the README.md to properly render on the repo. Please remove all the \\ 's the file will look like this: After pasting in the window, your file should look like this: We now type git init -b main Which will return Initialized empty Git repository in /home/ad376/fastqc-docker/.git/ And then we type git add . && git commit -m \"initial commit\" which returns something like this. [main (root-commit) d1e421f] initial commit 3 files changed, 33 insertions(+) create mode 100644 Dockerfile create mode 100644 README.md create mode 100644 environment.yml We then use the gh repo create command to create the reposistory. gh repo create Which then prompts us to what we need to do -- important is that what we want to do is Push an existing local repository to GitHub . The tool's remainder defaults are acceptable, because we staged ourselves with the name of the directory. (eos) ad376@cloudshell:~/multiqc-docker$ gh repo create ? What would you like to do? Push an existing local repository to GitHub ? Path to local repository . ? Repository name multiqc-docker ? Description a container for the bioconda command multiqc ? Visibility Public \u2713 Created repository adeslatt/fastqc-docker on GitHub ? Add a remote? Yes ? What should the new remote be called? origin \u2713 Added remote https://github.com/adeslatt/multiqc-docker.git ? Would you like to push commits from the current branch to \"origin\"? Yes \u2713 Pushed commits to https://github.com/adeslatt/multiqc-docker.git Upon success your repository will be in place on your GitHub.","title":"Add this multiqc code to your GitHub repository"},{"location":"classes/day-3-containerization/building-dockerfiles/#recap","text":"We have created a Dockerfile using the Google Shell Editor We have created the environment.yml file also with the Google Shell Editor We have used the new command placed our desired function fastqc within the environment.yml file for the ultimate Docker container. We built the container for the application fastqc in the fastqc-docker subdirectory. We used GitHub command line tool gh to take our local repositories and put both up on GitHub. We inspected our images. Tomorrow, we will make our first workflow and show how to use these containers within our private repository on CAVATICA Return to the Agenda","title":"Recap"},{"location":"classes/day-3-containerization/creating-a-conda-environment/","text":"Creating a Conda Environment What is conda Conda is an application that we use to control our environment and manage our package installations. We will work today within a terminal window and the first thing I like to do is to make a clean environment window. This ensures that all of the assumptions we made about our environment are clear and explicit. Without this you may inadvertantly introduce dependicies that may complicate your building of your container and/or stitching together your workflow. When we build our container, we begin many of times using the conda environment made by the group that builds conda as a basis. This then does give us the application conda within our environment for our use like docker. Today we will be using a terminal window within a free resource, Google Shell We will walk through those steps today with our simple example. Anaconda Search Packages I regularly type this in my browser window to find the packages I need to install. Anaconda environment is growing by leaps and bounds and there are more things that can be done there that I have yet to explore. In addition to well vetted packages that can be installed, this large community resource provides tutorials, data science instructions as well as whole environments where you can work, explore and learn. For example, today we already showed you how to run a jupyterlab notebook, but you want to know more. Their site offers instruction on Jupyterlab basics for you to explore. Logging into Google Shell For this part of our demo/hands on, we will be using google shell. Navigate please to Google Shell It will ask you to login, please follow the prompts and do so. Upon success you will see a window like this: We have an editor as well as a prompt. If you have items in your Google Shell, you may need to clean up a bit. There is not much space in this free ephemeral instance . Cleaning up your space If you have items in your directory, lets clean up! So we have enough room for the next few lessons. Let's see what is there. For a shell to list the contents of a directory, you type: ls -l You can use Explain Shell to explain the meaning of the command. Explain Shell ls -l . You can see hidden directories with the following command. ls -la Again, look at Explain Shell to explain the meaning of the command. Explain Shell ls -la Installing conda We need to get the installation package for conda. We get this package from continuum analytics, we will be installing Miniconda . Miniconda is the minimal installer and all we need. Navigating to that location you will see details here: Which installation do we need? Typing uname -a will tell us the information about our operating system and hardware basics. uname -a We see that we are using the Linux operating system, the first day I spun up this shell was on March 16 and we see that it has the architecture x86_64 . A handy 64-bit operating system. Scroll down the window and hover your cursor over the Miniconda Linux 64-bit (sorry it is implicitly that this is the right installation for the x86-64 architecture. Right click and select copy link address . This gives you the unique resource locator (url) for the installation script we will be downloading onto the machine we are talking to with our google cloud shell window. Now we will use the routine wget to copy this installation script. To make sure we can do this, let's make sure that we have the tool available. To do that, we type: which wget In my case, it returns the full path /usr/bin/wget . If you do not have wget , we can install it with Anaconda provided packages. Now to install Miniconda, so that we can use conda to install any and all packages we need we do as follows: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh If you haven't changed directories, you can see now that this package Miniconda3-latest-Linux-x86_64.sh is now in the top directory. As we learned earlier, we can use the bash shell commnad ls to inspect the contents of our current directory. ls -l provides the full path details of our directory. We check our current working directory with the bash shell comand pwd as we learned earlier. pwd For my execution, it returns /home/ad376 . Now running the command ls -l we see the downloaded file Miniconda3-latest-Linux-x86-64.sh . ls -l returns: /home/ad376 ad376@cloudshell:~$ ls -l total 73892 -rw-r--r-- 1 ad376 ad376 75660608 Feb 15 19:07 Miniconda3-latest-Linux-x86_64.sh -rw-r--r-- 1 ad376 ad376 913 Mar 31 12:34 README-cloudshell.txt ad376@cloudshell:~$ To execute this script now and install the conda application, we type: bash Miniconda3-latest-Linux-x86_64.sh This will return: Welcome to Miniconda3 py39_4.11.0 In order to continue the installation process, please review the license agreement. Please, press ENTER to continue >>> press ENTER to continue and accept the terms and defaults as instructed. I used SPACE to accelerate the passing of the license - when you have time you should read the details of this license as it is interesting. The last question is: Do you accept the license terms? [yes|no] [no] >>> yes It returns now: Miniconda3 will now be installed into this location: /home/ad376/miniconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below [/home/ad376/miniconda3] >>> Accept the default location to install. Next you will be asked Do you wish the installer to initialize Miniconda3 by running conda init? [yes|no] [no] >>> yes Please answer yes to this question. When finished we can see that the file is now in our execution path is the conda application. While conda now has been installed, to activate the use of conda we need to reboot our shell environment. The command exec -l bash will execute this restart of the bash shell. exec -l bash Now we can see the result: ad376@cloudshell:~$ exec -l bash Welcome to Cloud Shell! Type \"help\" to get started. To set your Cloud Platform project in this session use \u201cgcloud config set project [PROJECT_ID]\u201d (base) ad376@cloudshell:~$ And we can see now we have the conda application. which conda returns: /home/ad376/miniconda3/condabin/conda Creating a clean environment Now that we have conda we can create an environment that will keep our workspace clean. We can name it whatever we would like, since this is the Elements of Style course, I choose to name my environment eos for short. We create an environment as follows: conda create -n eos -y You can see now that we now have the option to activate the environment # # To activate this environment, use # # $ conda activate eos # # To deactivate an active environment, use # # $ conda deactivate Now that we have created a clean environment, we follow the directions and activate the environment, that is enter this new clean environment. conda activate eos We will see next to our name in the google shell, that it should look something like this: (base) ad376@cloudshell:~$ conda activate eos (eos) ad376@cloudshell:~$ The (eos) indicating that we are in that environment. You can list all your environments list all your environments with the following command: conda env list Recap We have learned in this lesson We started a bash shell environment using our browser and the shell We explored the open community-based resources offered through Anaconda We used wget available within the shell and got the installation package necessary to install conda We installed conda and We used numerous command line arguments We used conda to create a new clean environment. We now move on to build Docker images which we will use in a workflow. Return to the Agenda","title":"Creating a `Conda` Environment"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#creating-a-conda-environment","text":"","title":"Creating a Conda Environment"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#what-is-conda","text":"Conda is an application that we use to control our environment and manage our package installations. We will work today within a terminal window and the first thing I like to do is to make a clean environment window. This ensures that all of the assumptions we made about our environment are clear and explicit. Without this you may inadvertantly introduce dependicies that may complicate your building of your container and/or stitching together your workflow. When we build our container, we begin many of times using the conda environment made by the group that builds conda as a basis. This then does give us the application conda within our environment for our use like docker. Today we will be using a terminal window within a free resource, Google Shell We will walk through those steps today with our simple example.","title":"What is conda"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#anaconda-search-packages","text":"I regularly type this in my browser window to find the packages I need to install. Anaconda environment is growing by leaps and bounds and there are more things that can be done there that I have yet to explore. In addition to well vetted packages that can be installed, this large community resource provides tutorials, data science instructions as well as whole environments where you can work, explore and learn. For example, today we already showed you how to run a jupyterlab notebook, but you want to know more. Their site offers instruction on Jupyterlab basics for you to explore.","title":"Anaconda Search Packages"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#logging-into-google-shell","text":"For this part of our demo/hands on, we will be using google shell. Navigate please to Google Shell It will ask you to login, please follow the prompts and do so. Upon success you will see a window like this: We have an editor as well as a prompt. If you have items in your Google Shell, you may need to clean up a bit. There is not much space in this free ephemeral instance .","title":"Logging into Google Shell"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#cleaning-up-your-space","text":"If you have items in your directory, lets clean up! So we have enough room for the next few lessons. Let's see what is there. For a shell to list the contents of a directory, you type: ls -l You can use Explain Shell to explain the meaning of the command. Explain Shell ls -l . You can see hidden directories with the following command. ls -la Again, look at Explain Shell to explain the meaning of the command. Explain Shell ls -la","title":"Cleaning up your space"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#installing-conda","text":"We need to get the installation package for conda. We get this package from continuum analytics, we will be installing Miniconda . Miniconda is the minimal installer and all we need. Navigating to that location you will see details here: Which installation do we need? Typing uname -a will tell us the information about our operating system and hardware basics. uname -a We see that we are using the Linux operating system, the first day I spun up this shell was on March 16 and we see that it has the architecture x86_64 . A handy 64-bit operating system. Scroll down the window and hover your cursor over the Miniconda Linux 64-bit (sorry it is implicitly that this is the right installation for the x86-64 architecture. Right click and select copy link address . This gives you the unique resource locator (url) for the installation script we will be downloading onto the machine we are talking to with our google cloud shell window. Now we will use the routine wget to copy this installation script. To make sure we can do this, let's make sure that we have the tool available. To do that, we type: which wget In my case, it returns the full path /usr/bin/wget . If you do not have wget , we can install it with Anaconda provided packages. Now to install Miniconda, so that we can use conda to install any and all packages we need we do as follows: wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh If you haven't changed directories, you can see now that this package Miniconda3-latest-Linux-x86_64.sh is now in the top directory. As we learned earlier, we can use the bash shell commnad ls to inspect the contents of our current directory. ls -l provides the full path details of our directory. We check our current working directory with the bash shell comand pwd as we learned earlier. pwd For my execution, it returns /home/ad376 . Now running the command ls -l we see the downloaded file Miniconda3-latest-Linux-x86-64.sh . ls -l returns: /home/ad376 ad376@cloudshell:~$ ls -l total 73892 -rw-r--r-- 1 ad376 ad376 75660608 Feb 15 19:07 Miniconda3-latest-Linux-x86_64.sh -rw-r--r-- 1 ad376 ad376 913 Mar 31 12:34 README-cloudshell.txt ad376@cloudshell:~$ To execute this script now and install the conda application, we type: bash Miniconda3-latest-Linux-x86_64.sh This will return: Welcome to Miniconda3 py39_4.11.0 In order to continue the installation process, please review the license agreement. Please, press ENTER to continue >>> press ENTER to continue and accept the terms and defaults as instructed. I used SPACE to accelerate the passing of the license - when you have time you should read the details of this license as it is interesting. The last question is: Do you accept the license terms? [yes|no] [no] >>> yes It returns now: Miniconda3 will now be installed into this location: /home/ad376/miniconda3 - Press ENTER to confirm the location - Press CTRL-C to abort the installation - Or specify a different location below [/home/ad376/miniconda3] >>> Accept the default location to install. Next you will be asked Do you wish the installer to initialize Miniconda3 by running conda init? [yes|no] [no] >>> yes Please answer yes to this question. When finished we can see that the file is now in our execution path is the conda application. While conda now has been installed, to activate the use of conda we need to reboot our shell environment. The command exec -l bash will execute this restart of the bash shell. exec -l bash Now we can see the result: ad376@cloudshell:~$ exec -l bash Welcome to Cloud Shell! Type \"help\" to get started. To set your Cloud Platform project in this session use \u201cgcloud config set project [PROJECT_ID]\u201d (base) ad376@cloudshell:~$ And we can see now we have the conda application. which conda returns: /home/ad376/miniconda3/condabin/conda","title":"Installing conda"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#creating-a-clean-environment","text":"Now that we have conda we can create an environment that will keep our workspace clean. We can name it whatever we would like, since this is the Elements of Style course, I choose to name my environment eos for short. We create an environment as follows: conda create -n eos -y You can see now that we now have the option to activate the environment # # To activate this environment, use # # $ conda activate eos # # To deactivate an active environment, use # # $ conda deactivate Now that we have created a clean environment, we follow the directions and activate the environment, that is enter this new clean environment. conda activate eos We will see next to our name in the google shell, that it should look something like this: (base) ad376@cloudshell:~$ conda activate eos (eos) ad376@cloudshell:~$ The (eos) indicating that we are in that environment. You can list all your environments list all your environments with the following command: conda env list","title":"Creating a clean environment"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#recap","text":"We have learned in this lesson We started a bash shell environment using our browser and the shell We explored the open community-based resources offered through Anaconda We used wget available within the shell and got the installation package necessary to install conda We installed conda and We used numerous command line arguments We used conda to create a new clean environment. We now move on to build Docker images which we will use in a workflow.","title":"Recap"},{"location":"classes/day-3-containerization/creating-a-conda-environment/#return-to-the-agenda","text":"","title":"Return to the Agenda"},{"location":"classes/day-3-containerization/day-3-containerization/","text":"Agenda for the Day 3: Containerization with Environment Control Time (UTC) Programme 11.00 - 11.30 Recap on the Volcano Plot and agenda for the day 11:30 - 12:00 Creating a conda environment 12.00 - 12.10 Short break 12:10 - 12:50 Building Dockerfiles 12.50 - 13:00 Wrap up and overview of next days topics Additional Resources Anaconda Gallery Anaconda Open Data Science Anaconda Open Source Anaconda Packages Search Conda Docker File Documentation Google Shell Who is Anaconda","title":"Day 3 - Containerization"},{"location":"classes/day-3-containerization/day-3-containerization/#agenda-for-the-day-3-containerization-with-environment-control","text":"Time (UTC) Programme 11.00 - 11.30 Recap on the Volcano Plot and agenda for the day 11:30 - 12:00 Creating a conda environment 12.00 - 12.10 Short break 12:10 - 12:50 Building Dockerfiles 12.50 - 13:00 Wrap up and overview of next days topics","title":"Agenda for the Day 3: Containerization with Environment Control"},{"location":"classes/day-3-containerization/day-3-containerization/#additional-resources","text":"Anaconda Gallery Anaconda Open Data Science Anaconda Open Source Anaconda Packages Search Conda Docker File Documentation Google Shell Who is Anaconda","title":"Additional Resources"},{"location":"classes/day-3-containerization/recap-workspace-setup/","text":"Let's Recap Day 1 Introduced The Elements of Style approach On-boarded to the Cavatica platform and started Data Studio which is a JupyterLab Notebook Inspected the exampleVolcanoPlot.ipynb which is a JupyterLab Notebook running an R Kernal Day 2 Introduced the concepts of Git , GitHub Learned about version control , attributions , and how GitHub enables a Lab Notebook concept, preparing us to better use the vast and amazing capabilities available to us in the managed platform environment, Cavatica with access to enormous power and storage via Elastic Compute and the growing resources available on the cloud. We Forked a repository We Synced a repository We made a pull request . Let's revisit Reading Data and Plotting in R There were two missing steps to successfully run the exampleVolcanoPlot . The Reading-data-and-plotting-in-R.ipynb , a JupyterLab Notebook running an R kernal needed three items: the data found in Zenodo at the DOI (Data Object Identifier) location https://doi.org/10.5281/zenodo.7510587 . to have run the command to install required dependencies on the machine where the notebook was running *rscript install_dependencies.R* . and finally to declare one of the libraries installed that was not yet declared: *library(rlang)* . Those changes were updated in the original repository https://github.com/adeslatt/exampleVolcanoPlotNotebook . And if you have synchronized with the repository you Forked your repository from https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook , your repository is now up-to-date. Let's Read Data and Plot in R ! Go ahead and hit and lets get started. Give us a in the Zoom chat if all is going ok. You will soon see the JupyterLab Launch pad - which as we know will stay alive for about 30 minutes. Navigate to the right directory From the command line , navigate to the location where you git clone 'd your personal fork of the NIH-NICHD repository. cd adeslatt Now check the status of the repository with the command git status git status Uh-oh there is a problem! That's ok we were not quite in the right directory. You can look around where you are by using a command we learned yesterday ls -l ls -l Let's go down one more level to where the stuff we checked out of the repository actually is: cd exampleVolcanoPlotNotebook To check the status and to check the difference between what is in your directory and what is stored in your repository you use two commands git status and git diff git status followed by git diff Now in my case the repository had its own changes, but when I looked at the repository on the web site, they were identical. So I have decided I simply want to remove this directory with this work. I know it seems scary to do that, but with my changes in GitHub I have nothing to worry about. We are going to use another new command cd .. . In a terminal window there are shortcuts. Programmers like to use as few characters as possible to accomplish their work, including moving around in a terminal window. Over time you will also not only grow accustomed to these handy short cuts, but you will enjoy them as well. To look at your current directory you can use . . To look at the parent directory (what we call the directory above the one we are in, you can use .. . Go ahead and look around your current directory, by typing the unnecessary command cd . but I am using it to demonstrate a point (ha-ha). cd . ls -l But what we wanted to do is move up to the parent directoery. cd .. ls -l Ok now I want to remove the entire directory. Because I had local and changes at the master and really did not need to or desire to worry about how to clean it up. Easiest is to wipe the slate clean and get a fresh copy. We will use rm together with the options -f for force and -R for recursively delete all the subdirectories inside as well. You can go into ExplainShell to see this information for yourself. I also will get my clean repository by going to the code button and getting a copy by copying with the right protocol, * HTTPS* the link for git to use rm -f -R git clone https://github.com/adeslatt/exampleVolcanoPlotNotebook.git Now again open the notebook. Clear the outputs and restart the Kernel. Confirm And re-run! Now let's go back to our course. Return to the Agenda","title":"Recap workspace setup"},{"location":"classes/day-3-containerization/recap-workspace-setup/#lets-recap","text":"Day 1 Introduced The Elements of Style approach On-boarded to the Cavatica platform and started Data Studio which is a JupyterLab Notebook Inspected the exampleVolcanoPlot.ipynb which is a JupyterLab Notebook running an R Kernal Day 2 Introduced the concepts of Git , GitHub Learned about version control , attributions , and how GitHub enables a Lab Notebook concept, preparing us to better use the vast and amazing capabilities available to us in the managed platform environment, Cavatica with access to enormous power and storage via Elastic Compute and the growing resources available on the cloud. We Forked a repository We Synced a repository We made a pull request .","title":"Let's Recap"},{"location":"classes/day-3-containerization/recap-workspace-setup/#lets-revisit-reading-data-and-plotting-in-r","text":"There were two missing steps to successfully run the exampleVolcanoPlot . The Reading-data-and-plotting-in-R.ipynb , a JupyterLab Notebook running an R kernal needed three items: the data found in Zenodo at the DOI (Data Object Identifier) location https://doi.org/10.5281/zenodo.7510587 . to have run the command to install required dependencies on the machine where the notebook was running *rscript install_dependencies.R* . and finally to declare one of the libraries installed that was not yet declared: *library(rlang)* . Those changes were updated in the original repository https://github.com/adeslatt/exampleVolcanoPlotNotebook . And if you have synchronized with the repository you Forked your repository from https://github.com/NIH-NICHD/exampleVolcanoPlotNotebook , your repository is now up-to-date.","title":"Let's revisit Reading Data and Plotting in R"},{"location":"classes/day-3-containerization/recap-workspace-setup/#lets-read-data-and-plot-in-r","text":"Go ahead and hit and lets get started. Give us a in the Zoom chat if all is going ok. You will soon see the JupyterLab Launch pad - which as we know will stay alive for about 30 minutes.","title":"Let's Read Data and Plot in R !"},{"location":"classes/day-3-containerization/recap-workspace-setup/#navigate-to-the-right-directory","text":"From the command line , navigate to the location where you git clone 'd your personal fork of the NIH-NICHD repository. cd adeslatt Now check the status of the repository with the command git status git status Uh-oh there is a problem! That's ok we were not quite in the right directory. You can look around where you are by using a command we learned yesterday ls -l ls -l Let's go down one more level to where the stuff we checked out of the repository actually is: cd exampleVolcanoPlotNotebook To check the status and to check the difference between what is in your directory and what is stored in your repository you use two commands git status and git diff git status followed by git diff Now in my case the repository had its own changes, but when I looked at the repository on the web site, they were identical. So I have decided I simply want to remove this directory with this work. I know it seems scary to do that, but with my changes in GitHub I have nothing to worry about. We are going to use another new command cd .. . In a terminal window there are shortcuts. Programmers like to use as few characters as possible to accomplish their work, including moving around in a terminal window. Over time you will also not only grow accustomed to these handy short cuts, but you will enjoy them as well. To look at your current directory you can use . . To look at the parent directory (what we call the directory above the one we are in, you can use .. . Go ahead and look around your current directory, by typing the unnecessary command cd . but I am using it to demonstrate a point (ha-ha). cd . ls -l But what we wanted to do is move up to the parent directoery. cd .. ls -l Ok now I want to remove the entire directory. Because I had local and changes at the master and really did not need to or desire to worry about how to clean it up. Easiest is to wipe the slate clean and get a fresh copy. We will use rm together with the options -f for force and -R for recursively delete all the subdirectories inside as well. You can go into ExplainShell to see this information for yourself. I also will get my clean repository by going to the code button and getting a copy by copying with the right protocol, * HTTPS* the link for git to use rm -f -R git clone https://github.com/adeslatt/exampleVolcanoPlotNotebook.git Now again open the notebook. Clear the outputs and restart the Kernel. Confirm And re-run! Now let's go back to our course. Return to the Agenda","title":"Navigate to the right directory"},{"location":"classes/day-4-workflow-development/day-4-workflow-development/","text":"Agenda for the Day 4: Workflow Development Time (UTC) Programme 11.00 - 11.30 :wave: Welcome and Preamble to Building our Workflow 11:10 - 12.00 Building A Nextflow Script 12.00 - 12.10 :coffee: Short break 12:10 - 12:40 Building A CWL Script 12.40 - 11.50 Shared elements across workflow languages 12.50 - 13:00 :raised_hand: Wrap up and overview of next days topics Additional Resources Nextflow Nextflow Basic pipeline with detail description of the file Nextflow Mixing Programming Languages In The Workflow Nextflow Blast Example Nextflow Community Based Pipelines Common Workflow Language Common Workflow Langage How-to","title":"Day 4 - Workflow development"},{"location":"classes/day-4-workflow-development/day-4-workflow-development/#agenda-for-the-day-4-workflow-development","text":"Time (UTC) Programme 11.00 - 11.30 :wave: Welcome and Preamble to Building our Workflow 11:10 - 12.00 Building A Nextflow Script 12.00 - 12.10 :coffee: Short break 12:10 - 12:40 Building A CWL Script 12.40 - 11.50 Shared elements across workflow languages 12.50 - 13:00 :raised_hand: Wrap up and overview of next days topics","title":"Agenda for the Day 4: Workflow Development"},{"location":"classes/day-4-workflow-development/day-4-workflow-development/#additional-resources","text":"Nextflow Nextflow Basic pipeline with detail description of the file Nextflow Mixing Programming Languages In The Workflow Nextflow Blast Example Nextflow Community Based Pipelines Common Workflow Language Common Workflow Langage How-to","title":"Additional Resources"},{"location":"classes/day-5-workflow-execution/day-5-workflow-execution/","text":"Agenda for the Day 5: Workflow Execution Time (UTC) Programme 11.00 - 11.20 :wave: Recap of the week so far 11.20 - 12.00 Working with Apps on the CAVATICA 12.00 - 12.10 :coffee: Short break 12:30 - 12.45 GitHub Actions to build, test and deposit container images 12.45 - 13:00 :wave: Cloud Credits Review, Course Survey and Wrap up Additional resources: A small example - picard FilterSamReads A Published Large Example: Zenodo","title":"Day 5 - Workflow execution"},{"location":"classes/day-5-workflow-execution/day-5-workflow-execution/#agenda-for-the-day-5-workflow-execution","text":"Time (UTC) Programme 11.00 - 11.20 :wave: Recap of the week so far 11.20 - 12.00 Working with Apps on the CAVATICA 12.00 - 12.10 :coffee: Short break 12:30 - 12.45 GitHub Actions to build, test and deposit container images 12.45 - 13:00 :wave: Cloud Credits Review, Course Survey and Wrap up","title":"Agenda for the Day 5: Workflow Execution"},{"location":"classes/day-5-workflow-execution/day-5-workflow-execution/#additional-resources","text":"A small example - picard FilterSamReads A Published Large Example: Zenodo","title":"Additional resources:"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/","text":"Kids First, INCLUDE and Cavatica Account Registrations Today in this pre-training workshop, we will spend some time to get everyone registered for a few accounts. By now, we have a GitHub account, an ORCID and hopefully an ERA-Commons ID. This last one will allow us to take advantage of the NIH Researcher Authentication Service. In this portion of the class, we will be registering with Kids First as well as Cavatica and connecting these two accounts together. Beginning with Kids First Account Some of the data within the Kids First Data Resource Center Hub is registered tier access. What that means is that you simply need to register and the data are available to you. Creating a Kids First DRC Account Please create your account within the Kids First Data Resource Portal. Navigate to Kids First DRC First screen you will see is: From here you can navigate to either CREATE YOUR ACCOUNT or PORTAL LOGIN Let's click CREATE YOUR ACCOUNT Select a Connection Service A Connection service is used to manage your identity. Let's go ahead and Select a connection service . We have three choices: We have today created our ORCID and perhaps our ERACommons ID and we have the option of selecting Google. Each have their advantages and disadvantages. I'd like to steer you towards the NIH Researcher Authentication Service. The reason is that this is the service that will know what data sets you are authorized to access. Protecting Human Data - Introducing GA4GH Pediatric data, human data are sensitve data. To protect the use of the data, we have data access committees. Data Access Committees is a world-wide phenomenon. In this genomic era, where medicine and research involves many types of omics data, protecting these data means that one registers their intent and that intent is then documented. While today this is an often manual process, we do imagine a future where there is more automation involved, so the process becomes a bit more smoothed out. In the meantime, the NIH Researcher Authentication Service, is one step closer to the ideal. First there is the management of your identity, next is the management of what datasets you have access. The Global Alliance for Genomics & Health (GA4GH) is a policy-framing and technical standards-setting organization, seeking to enable responsible genomic data sharing within a human rights framework. I encourage you to stay abreast with this organization and the emerging policies they are promoting. Logging in with the NIH Researcher Authentication Service Let's choose the NIH Researcher Authentication Service Join Kids First Data Resource Portal You are now asked to enter basic Information after you have connected with Kids First. Enter the information that describes you best. The example on the website is a fictious Researcher. You are asked to agree to adhere to the NIH Genomic Data User Code of Conduct . Some of those terms are recapitulated on the page. Kids First Portal Dashboard Once these terms are accepted - you will be presented with a Dashboard And we are now ready to connect to Cavatica. Kids First Connect to Cavatica From the Kids First new user dashboard, we see how to connect to Cavatica, we see there are three steps. Creating an INCLUDE Data Hub Account Some of the data within the INCLUDE Data Hub is registered tier access. What that means is that you simply need to register and the data are available to you. Navigate to INCLUDE Data Hub First screen you will see is: From here you can navigate to either Login or Sign up Let's click Sign Up Sign up choices At this time, you can sign up with either Google or ORCID. Either works: Let's choose ORCID, when it becomes available, you will also be able to sign in with your ERACommons ID. We see now the Dashboard and we are now ready to connect to Cavatica. INCLUDE Connect with Cavatica From the INCLUDE dashboard, we see how to connect to Cavatica Using the Connect button on the left, we login to with the Researcher Authorization Service. The data are at this time mediated by a data service, Gen3 Now authenticated, the INCLUDE project to see what studies you are authorized to see. The portal then will allow you to browse these files and push them onto Cavatica. There are other courses that teach you how to do these steps. We are focused on building the literacy here of how to use and build your own tools with Cavatica in as much as possible a platform agnostic manner. Next, we connect using the Connect button on the right to Cavatica. And we are now ready to go to Cavatica See further documentation: Cavatica Account Login Creation Documentation Kids First Getting Started INCLUDE Help Recap of what we have done today Registered with Kids First Data Resource Center Registered with INCLUDE Registered with Cavatica Connected the Kids First DRC with the Cavatica Platform and Researcher Authorization Service Connected the INCLUDE Data Hub with Cavatica Platform and Researcher Authorization Service","title":"Pre training creating kids first and other accounts"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#kids-first-include-and-cavatica-account-registrations","text":"Today in this pre-training workshop, we will spend some time to get everyone registered for a few accounts. By now, we have a GitHub account, an ORCID and hopefully an ERA-Commons ID. This last one will allow us to take advantage of the NIH Researcher Authentication Service. In this portion of the class, we will be registering with Kids First as well as Cavatica and connecting these two accounts together.","title":"Kids First, INCLUDE  and Cavatica Account Registrations"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#beginning-with-kids-first-account","text":"Some of the data within the Kids First Data Resource Center Hub is registered tier access. What that means is that you simply need to register and the data are available to you.","title":"Beginning with Kids First Account"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#creating-a-kids-first-drc-account","text":"Please create your account within the Kids First Data Resource Portal. Navigate to Kids First DRC First screen you will see is: From here you can navigate to either CREATE YOUR ACCOUNT or PORTAL LOGIN Let's click CREATE YOUR ACCOUNT","title":"Creating a Kids First DRC Account"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#select-a-connection-service","text":"A Connection service is used to manage your identity. Let's go ahead and Select a connection service . We have three choices: We have today created our ORCID and perhaps our ERACommons ID and we have the option of selecting Google. Each have their advantages and disadvantages. I'd like to steer you towards the NIH Researcher Authentication Service. The reason is that this is the service that will know what data sets you are authorized to access.","title":"Select a Connection Service"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#protecting-human-data-introducing-ga4gh","text":"Pediatric data, human data are sensitve data. To protect the use of the data, we have data access committees. Data Access Committees is a world-wide phenomenon. In this genomic era, where medicine and research involves many types of omics data, protecting these data means that one registers their intent and that intent is then documented. While today this is an often manual process, we do imagine a future where there is more automation involved, so the process becomes a bit more smoothed out. In the meantime, the NIH Researcher Authentication Service, is one step closer to the ideal. First there is the management of your identity, next is the management of what datasets you have access. The Global Alliance for Genomics & Health (GA4GH) is a policy-framing and technical standards-setting organization, seeking to enable responsible genomic data sharing within a human rights framework. I encourage you to stay abreast with this organization and the emerging policies they are promoting.","title":"Protecting Human Data - Introducing GA4GH"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#logging-in-with-the-nih-researcher-authentication-service","text":"Let's choose the NIH Researcher Authentication Service","title":"Logging in with the NIH Researcher Authentication Service"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#join-kids-first-data-resource-portal","text":"You are now asked to enter basic Information after you have connected with Kids First. Enter the information that describes you best. The example on the website is a fictious Researcher. You are asked to agree to adhere to the NIH Genomic Data User Code of Conduct . Some of those terms are recapitulated on the page.","title":"Join Kids First Data Resource Portal"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#kids-first-portal-dashboard","text":"Once these terms are accepted - you will be presented with a Dashboard And we are now ready to connect to Cavatica.","title":"Kids First Portal Dashboard"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#kids-first-connect-to-cavatica","text":"From the Kids First new user dashboard, we see how to connect to Cavatica, we see there are three steps.","title":"Kids First Connect to Cavatica"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#creating-an-include-data-hub-account","text":"Some of the data within the INCLUDE Data Hub is registered tier access. What that means is that you simply need to register and the data are available to you. Navigate to INCLUDE Data Hub First screen you will see is: From here you can navigate to either Login or Sign up Let's click Sign Up","title":"Creating an INCLUDE Data Hub Account"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#sign-up-choices","text":"At this time, you can sign up with either Google or ORCID. Either works: Let's choose ORCID, when it becomes available, you will also be able to sign in with your ERACommons ID. We see now the Dashboard and we are now ready to connect to Cavatica.","title":"Sign up choices"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#include-connect-with-cavatica","text":"From the INCLUDE dashboard, we see how to connect to Cavatica Using the Connect button on the left, we login to with the Researcher Authorization Service. The data are at this time mediated by a data service, Gen3 Now authenticated, the INCLUDE project to see what studies you are authorized to see. The portal then will allow you to browse these files and push them onto Cavatica. There are other courses that teach you how to do these steps. We are focused on building the literacy here of how to use and build your own tools with Cavatica in as much as possible a platform agnostic manner. Next, we connect using the Connect button on the right to Cavatica. And we are now ready to go to Cavatica See further documentation: Cavatica Account Login Creation Documentation Kids First Getting Started INCLUDE Help","title":"INCLUDE Connect with Cavatica"},{"location":"classes/pre-training/pre-training-creating-kids-first-and-other-accounts/#recap-of-what-we-have-done-today","text":"Registered with Kids First Data Resource Center Registered with INCLUDE Registered with Cavatica Connected the Kids First DRC with the Cavatica Platform and Researcher Authorization Service Connected the INCLUDE Data Hub with Cavatica Platform and Researcher Authorization Service","title":"Recap of what we have done today"},{"location":"classes/pre-training/pre-training/","text":"Agenda for Pre-Training Workshop Time (UTC) Programme 11.00 - 11.10 Welcome Address, Motivation, Why we are here 11.10 - 11.30 Let's Register! 11.30 - 11.50 Kids First, INCLUDE and CAVATICA Registrations 11.50 - 12:00 Wrap up and overview of next days topics Quick Links: GitHub Zenodo ORCID Registration Now! ERA Commons ID INCLUDE Data Hub Kids First Data Resource Portal Cavatica by Velsera (formerly Seven Bridges) Additional Resources: NIH Kids First Cloud Credit Program Overview NIH INCLUDE Cloud Credit Program Overview How to apply for Kids First Cloud Credits How to apply for INCLUDE Cloud Credits Joint Kids First and INCLUDE with Velsera (formerly Seven Bridges) Monthly User Support Office Hours - every third Thursday of the month","title":"Pre-training"},{"location":"classes/pre-training/pre-training/#agenda-for-pre-training-workshop","text":"Time (UTC) Programme 11.00 - 11.10 Welcome Address, Motivation, Why we are here 11.10 - 11.30 Let's Register! 11.30 - 11.50 Kids First, INCLUDE and CAVATICA Registrations 11.50 - 12:00 Wrap up and overview of next days topics","title":"Agenda for Pre-Training Workshop"},{"location":"classes/pre-training/pre-training/#quick-links","text":"GitHub Zenodo ORCID Registration Now! ERA Commons ID INCLUDE Data Hub Kids First Data Resource Portal Cavatica by Velsera (formerly Seven Bridges)","title":"Quick Links:"},{"location":"classes/pre-training/pre-training/#additional-resources","text":"NIH Kids First Cloud Credit Program Overview NIH INCLUDE Cloud Credit Program Overview How to apply for Kids First Cloud Credits How to apply for INCLUDE Cloud Credits Joint Kids First and INCLUDE with Velsera (formerly Seven Bridges) Monthly User Support Office Hours - every third Thursday of the month","title":"Additional Resources:"}]}